{"meta":{"title":"Questionable Engineering","subtitle":"","description":"A collection of pointless, random, and probaly bad ideas","author":"John Grun","url":"http://questionableengineering.com","root":"/"},"pages":[{"title":"categories","date":"2020-09-04T15:28:51.000Z","updated":"2020-09-04T15:28:51.095Z","comments":true,"path":"categories/index.html","permalink":"http://questionableengineering.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2020-09-04T15:30:57.000Z","updated":"2022-08-18T16:01:53.687Z","comments":true,"path":"about/index.html","permalink":"http://questionableengineering.com/about/index.html","excerpt":"","text":"This blog was primarily created as a place to keep my project notes."},{"title":"tags","date":"2020-09-04T15:30:38.000Z","updated":"2020-09-04T15:30:38.928Z","comments":true,"path":"tags/index.html","permalink":"http://questionableengineering.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Home Assistant Setup on Kubernetes","slug":"Home-Assistant-Setup-on-Kubernetes","date":"2023-10-03T04:22:39.000Z","updated":"2024-07-15T00:16:46.743Z","comments":true,"path":"2023/10/03/Home-Assistant-Setup-on-Kubernetes/","link":"","permalink":"http://questionableengineering.com/2023/10/03/Home-Assistant-Setup-on-Kubernetes/","excerpt":"","text":"Install the k8s cluster if not alreadly installed12microk8s enable dns storage helm3microk8s status Add the helm Repo1microk8s helm repo add alekc-charts https:&#x2F;&#x2F;charts.alekc.dev&#x2F; Install the helm chart1microk8s helm install home-assistant alekc-charts&#x2F;home-assistant Check the status of the installed application.12microk8s statusmicrok8s kubectl describe pods Service yaml fileWe need to expose the service to the outside world.Thankfully microk8s has a built in loadbalancer called metallb 1nano home-assistant-service.yaml Replace port_number with the real port number 12345678910111213141516apiVersion: v1kind: Servicemetadata: name: home-assistant-servicespec: type: LoadBalancer selector: app.kubernetes.io&#x2F;name: home-assistant ports: - name: webportal protocol: TCP port: port_number targetPort: port_number externalIPs: - A.B.C.D Apply the service1microk8s kubectl apply -f .&#x2F;home-assistant-service.yaml Confirm the service is active1microk8s kubectl describe services home-assistant-service Firewall RulesThis assumes you are using ufw.ufw is bascally a wrapper for IPTABLES. If you have ever used IPTABLES before you understand why ufw exists.Replace port_number with the real port number 1234sudo ufw default allow routed sudo ufw allow from A.B.C.0&#x2F;24 to any port port_number proto tcpsudo ufw allow from D.E.F.0&#x2F;24 to any port port_number proto tcpsudo ufw status","categories":[],"tags":[]},{"title":"Jellyfin Media Server Setup on Kubernetes","slug":"Jellyfin-Media-Server-Setup-on-Kubernetes","date":"2023-08-31T02:52:08.000Z","updated":"2024-07-15T00:17:14.425Z","comments":true,"path":"2023/08/30/Jellyfin-Media-Server-Setup-on-Kubernetes/","link":"","permalink":"http://questionableengineering.com/2023/08/30/Jellyfin-Media-Server-Setup-on-Kubernetes/","excerpt":"","text":"Install the k8s cluster if not alreadly installed12microk8s enable dns storage helm3microk8s status Add the Truecharts Repo1microk8s helm repo add truecharts https:&#x2F;&#x2F;charts.truecharts.org&#x2F; Pull the helm chart1microk8s helm pull truecharts&#x2F;jellyfin To enable DLNA we need to enable access to the host network.Host Access addon 1microk8s enable host-access:ip&#x3D;A.B.C.D Modify the values.yaml file to set the configuration123tar -xvf .&#x2F;jellyfin-*.tgzcd jellyfincat values.yamls Make the data directoryAPPLICATION_ROOT_DIRECTORY: Path where the application config and data files are stored 1234mkdir -p APPLICATION_ROOT_DIRECTORY&#x2F;configmkdir -p APPLICATION_ROOT_DIRECTORY&#x2F;datasudo chown -R 777 APPLICATION_ROOT_DIRECTORY Create Persistent Volumes and Persistent Volume ClaimsPersistent VolumesConfigjellyfin-config-pv.yaml 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: jellyfin-config-pvspec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: APPLICATION_ROOT_DIRECTORY&#x2F;config # This must exist on the host nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io&#x2F;hostname operator: In values: - dionysus 1microk8s kubectl apply -f jellyfin-config-pv.yaml Datajellyfin-data-pv.yaml 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: jellyfin-data-pvspec: capacity: storage: 5Ti volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: APPLICATION_ROOT_DIRECTORY&#x2F;data # This must exist on the host nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io&#x2F;hostname operator: In values: - dionysus 1microk8s kubectl apply -f jellyfin-data-pv.yaml 1microk8s kubectl get pv 123NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEjellyfin-config-pv 100Gi RWO Retain Bound default&#x2F;jellyfin-config-pvc local-storage 98sjellyfin-data-pv 5Ti RWO Retain Bound default&#x2F;jellyfin-data-pvc local-storage 78s Persistent Volume ClaimsConfigjellyfin-config-pvc.yamlBonds to jellyfin-config-pv 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: jellyfin-config-pvcspec: storageClassName: local-storage # Empty string must be explicitly set otherwise default StorageClass will be set accessModes: - ReadWriteOnce volumeName: jellyfin-config-pv resources: requests: storage: 100Gi 1microk8s kubectl apply -f jellyfin-config-pvc.yaml Datajellyfin-data-pvc.yamlBonds to jellyfin-data-pv 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: jellyfin-data-pvcspec: storageClassName: local-storage # Empty string must be explicitly set otherwise default StorageClass will be set accessModes: - ReadWriteOnce volumeName: jellyfin-data-pv resources: requests: storage: 5Ti 1microk8s kubectl apply -f jellyfin-data-pvc.yaml 1microk8s kubectl get pvc 123NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEjellyfin-config-pvc Bound jellyfin-config-pv 100Gi RWO local-storage 26sjellyfin-data-pvc Bound jellyfin-data-pv 5Ti RWO local-storage 12s Modify the values.yaml file to set the configurationWe’re going to set the jellyfin configuration within the container. 12cd jellyfinnano values.yaml DLNA_PORT: Port the dlna server will listen onAUTO_DISCOVER_PORT: DLNA autodiscover portWEB_GUI_PORT: Port to access the web gui 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115image: repository: tccr.io&#x2F;truecharts&#x2F;jellyfin pullPolicy: IfNotPresent tag: v10.8.10@sha256:d2c377ee7ea463110a1dda7eb1b231424ad05245aaf95744e76164bd2e593377broadcastProxyImage: repository: tccr.io&#x2F;truecharts&#x2F;socat pullPolicy: IfNotPresent tag: v1.7.4.4@sha256:2417f121a5fd08012c927bfb7cdedb00ce97e0ec53fbf38352be8c6f197f7294service: main: ports: main: port: WEB_GUI_PORT targetPort: WEB_GUI_PORT autodiscovery: enabled: true ports: autodiscovery: enabled: true protocol: udp port: AUTO_DISCOVER_PORT hostPort: AUTO_DISCOVER_PORT hostIP: A.B.C.D dlna: enabled: true ports: dlna: enabled: true protocol: udp port: DLNA_PORT hostPort: DLNA_PORT hosstIP: A.B.C.Dpersistence: config: enabled: true mountPath: &quot;&#x2F;config&quot; type: pvc existingClaim: jellyfin-config-pvc cache: enabled: true mountPath: &quot;&#x2F;cache&quot; type: &quot;emptyDir&quot; transcode: enabled: true mountPath: &quot;&#x2F;config&#x2F;transcodes&quot; type: &quot;emptyDir&quot; media: enabled: true mountPath: &#x2F;media type: pvc existingClaim: jellyfin-data-pvcportal: open: enabled: truesecurityContext: container: readOnlyRootFilesystem: falseworkload: main: podSpec: hostNetwork: true containers: main: env: JELLYFIN_PublishedServerUrl: &quot;&#123;&#123; $.Values.chartContext.APPURL &#125;&#125;&quot; broadcastproxy: enabled: false type: DaemonSet podSpec: hostNetwork: true # Proxy doesn&#39;t seem to respect the TERM signal, so by default # this ends up just hanging until the default grace period ends. # This is unnecesary since this workload only proxies autodiscovery # messages. terminationGracePeriodSeconds: 3 containers: broadcastproxy: enabled: true primary: true imageSelector: broadcastProxyImage securityContext: readOnlyRootFilesystem: true command: [&quot;&#x2F;bin&#x2F;sh&quot;] # Quite a lot going on here: # - Resolve Jellyfin&#39;s autodiscovery service IP from its FQDN via getent hosts # - Export the IP to &#96;$TARGET_IP&#96; # - Check &#96;$TARGET_IP&#96; is not empty (so we can crash if it is - will help to detect templating errors) # - Touch &#96;&#x2F;tmp&#x2F;healty&#96; to use with the readiness, liveness and startup probes # - Start socat in proxy mode # - On exit remove &#96;&#x2F;tmp&#x2F;healthy&#96; args: [&quot;-c&quot;, &quot;export TARGET_IP&#x3D;$(getent hosts &#39;&#123;&#123; printf \\&quot;%v-autodiscovery\\&quot; (include \\&quot;tc.v1.common.lib.chart.names.fullname\\&quot; $) &#125;&#125;&#39; | awk &#39;&#123; print $1 &#125;&#39;) &amp;&amp; [[ ! -z $TARGET_IP ]] &amp;&amp; touch &#x2F;tmp&#x2F;healthy &amp;&amp; socat UDP-LISTEN:AUTO_DISCOVER_PORT,fork,reu&gt; probes: readiness: enabled: true type: exec command: - cat - &#x2F;tmp&#x2F;healthy liveness: enabled: true type: exec command: - cat - &#x2F;tmp&#x2F;healthy startup: enabled: true type: exec command: - cat - &#x2F;tmp&#x2F;healthy# -- enable Jellyfin autodiscovery on LANautodiscovery: enabled: true Install the helm chart with our values.yaml file1microk8s helm install jellyfin truecharts&#x2F;jellyfin --values .&#x2F;values.yaml Check the status of the installed application.12microk8s statusmicrok8s kubectl describe pods Common ErrorsError from server (BadRequest): container in pod is waiting to start: ContainerCreatingYou probably need to change the permissions on the PV directory. This path is what is written in the PersistentVolume in the path variable. A quick chmod -R 777 to this path will most likely fix the issue. The conatiner should update the permissions once it runs. Service yaml fileSince we are using the hostnetwork we don’t need the an additional service file to route information. More information herehttps://medium.com/swlh/kubernetes-external-ip-service-type-5e5e9ad62fcd Firewall RulesThis assumes you are using ufw.ufw is bascally a wrapper for IPTABLES. If you have ever used IPTABLES before you understand why ufw exists.https://docs.syncthing.net/users/firewall.html 1sudo ufw default allow routed 123sudo ufw allow from A.B.C.0&#x2F;24 to any port WEB_GUI_PORT proto tcpsudo ufw allow from A.B.C.0&#x2F;24 to any port AUTO_DISCOVER_PORT proto udpsudo ufw allow from A.B.C.0&#x2F;24 to any port DLNA_PORT proto udp 1sudo ufw status Referenceshttps://kubernetes.io/docs/concepts/services-networking/service/","categories":[],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://questionableengineering.com/tags/Kubernetes/"},{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Microk8s","slug":"Microk8s","permalink":"http://questionableengineering.com/tags/Microk8s/"}]},{"title":"Zigbee2Mqtt Kubernetes Setup on Ubuntu","slug":"Zigbee2Mqtt-Kubernetes-Setup-on-Ubuntu","date":"2023-08-23T18:31:06.000Z","updated":"2024-07-15T00:24:08.849Z","comments":true,"path":"2023/08/23/Zigbee2Mqtt-Kubernetes-Setup-on-Ubuntu/","link":"","permalink":"http://questionableengineering.com/2023/08/23/Zigbee2Mqtt-Kubernetes-Setup-on-Ubuntu/","excerpt":"","text":"Install the k8s cluster if not alreadly installed12microk8s enable dns storage helm3microk8s status Pull the helm chart12microk8s helm repo add truecharts https:&#x2F;&#x2F;charts.truecharts.org&#x2F;microk8s helm pull truecharts&#x2F;zigbee2mqtt --version 7.0.30 Modify the values.yaml file to set the configuration123tar -xvf .&#x2F;zigbee2mqtt-*.tgzcd zigbee2mqttcat values.yamls 123456EXTERNAL_IP: A.B.C.DEXTERNAL_NETWORK: A.B.C.0SERVICE_PORT: Port used by the applicationZIGBEE_ADAPTER_IP: Ip address of the zigbee adapterZIGBEE_ZIGBEE_ADAPTER_PORT: Port used by the external zigbee adapterMQTT_PORT: PORT_C 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100image: repository: tccr.io&#x2F;truecharts&#x2F;zigbee2mqtt tag: v1.32.1@sha256:f3bcf64a1a538ce5636b7359bd4f0375f593c9408d561a1add9f3d2bed843bf3 pullPolicy: IfNotPresentservice: main: ports: main: port: SERVICE_PORTpersistence: data: enabled: true mountPath: &quot;&#x2F;data&quot; targetSelectAll: trueportal: open: enabled: truesecurityContext: container: runAsNonRoot: false readOnlyRootFilesystem: false runAsUser: 0 runAsGroup: 0workload: main: podSpec: initContainers: init-config: enabled: true imageSelector: image type: init env: ZIGBEE2MQTT_CONFIG_FRONTEND_PORT: &quot;&#123;&#123; .Values.service.main.ports.main.port &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_EXPIRIMENTAL_NEW_API: &quot;&#123;&#123; .Values.workload.main.podSpec.containers.main.env.ZIGBEE2MQTT_CONFIG_EXPIRIMENTAL_NEW_API &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_PERMIT_JOIN: &quot;&#123;&#123; .Values.workload.main.podSpec.containers.main.env.ZIGBEE2MQTT_CONFIG_PERMIT_JOIN &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_MQTT_SERVER: &quot;&#123;&#123; .Values.workload.main.podSpec.containers.main.env.ZIGBEE2MQTT_CONFIG_MQTT_SERVER &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_MQTT_USER: &quot;&#123;&#123; .Values.secret.ZIGBEE2MQTT_CONFIG_MQTT_USER &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_MQTT_PASSWORD: &quot;&#123;&#123; .Values.secret.ZIGBEE2MQTT_CONFIG_MQTT_PASSWORD &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_MQTT_BASE_TOPIC: &quot;&#123;&#123; .Values.workload.main.podSpec.containers.main.env.ZIGBEE2MQTT_CONFIG_MQTT_BASE_TOPIC &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_SERIAL_PORT: &quot;&#123;&#123; .Values.workload.main.podSpec.containers.main.env.ZIGBEE2MQTT_CONFIG_SERIAL_PORT &#125;&#125;&quot; ZIGBEE2MQTT_CONFIG_SERIAL_ADAPTER: &quot;&#123;&#123; .Values.workload.main.podSpec.containers.main.env.ZIGBEE2MQTT_CONFIG_SERIAL_ADAPTER &#125;&#125;&quot; USE_CUSTOM_CONFIG_FILE: &quot;&#123;&#123; .Values.workload.main.podSpec.containers.main.env.USE_CUSTOM_CONFIG_FILE &#125;&#125;&quot; command: - &#x2F;bin&#x2F;sh - -c args: - &gt; if [ -f &#x2F;data&#x2F;configuration.yaml ] || [ $&#123;USE_CUSTOM_CONFIG_FILE&#125; &#x3D;&#x3D; true ]; then echo &quot;Initial configuration exists or User selected to use custom configuration file. Skipping...&quot;; else echo &quot;Creating initial configuration&quot;; touch &#x2F;data&#x2F;configuration.yaml; echo &quot;# Configuration bellow will be always be overridden&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;# from environment settings on the Scale Apps UI.&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;# You however will not see this values change in the file.&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;# It&#39;s a generated file based on the values provided on initial install.&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;##########################################################&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;experimental:&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot; new_api: $ZIGBEE2MQTT_CONFIG_EXPIRIMENTAL_NEW_API&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;frontend:&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot; port: $ZIGBEE2MQTT_CONFIG_FRONTEND_PORT&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;permit_join: $ZIGBEE2MQTT_CONFIG_PERMIT_JOIN&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;mqtt:&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot; server: $ZIGBEE2MQTT_CONFIG_MQTT_SERVER&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot; base_topic: $ZIGBEE2MQTT_CONFIG_MQTT_BASE_TOPIC&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; if [ ! -z &quot;$ZIGBEE2MQTT_CONFIG_MQTT_USER&quot; ]; then echo &quot; user: $ZIGBEE2MQTT_CONFIG_MQTT_USER&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; fi; if [ ! -z &quot;$ZIGBEE2MQTT_CONFIG_MQTT_PASSWORD&quot; ]; then echo &quot; password: $ZIGBEE2MQTT_CONFIG_MQTT_PASSWORD&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; fi; echo &quot;serial:&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot; port: $ZIGBEE2MQTT_CONFIG_SERIAL_PORT&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot; adapter: $ZIGBEE2MQTT_CONFIG_SERIAL_ADAPTER&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &quot;##########################################################&quot; &gt;&gt; &#x2F;data&#x2F;configuration.yaml; echo &#39;Initial configuration file created at &quot;&#x2F;data&#x2F;configuration.yaml&quot;&#39;; fi; containers: main: env: ZIGBEE2MQTT_DATA: &quot;&#x2F;data&quot; ZIGBEE2MQTT_CONFIG_FRONTEND_PORT: &quot;&#123;&#123; .Values.service.main.ports.main.port &#125;&#125;&quot; # User defined USE_CUSTOM_CONFIG_FILE: false # This values are required for the autogenerated file to work. ZIGBEE2MQTT_CONFIG_EXPIRIMENTAL_NEW_API: false ZIGBEE2MQTT_CONFIG_PERMIT_JOIN: false ZIGBEE2MQTT_CONFIG_MQTT_SERVER: &quot;mqtt:&#x2F;&#x2F;mosquitto:MQTT_PORT&quot; ZIGBEE2MQTT_CONFIG_MQTT_BASE_TOPIC: &quot;zigbee2mqtt&quot; ZIGBEE2MQTT_CONFIG_SERIAL_PORT: &quot;tcp:&#x2F;&#x2F;ZIGBEE_ADAPTER_IP:ZIGBEE_ADAPTER_PORT&quot; ZIGBEE2MQTT_CONFIG_SERIAL_ADAPTER: &quot;auto&quot; ZIGBEE2MQTT_CONFIG_MQTT_USER: &quot;&quot; ZIGBEE2MQTT_CONFIG_MQTT_PASSWORD: &quot;&quot; Servicezigbee2mqtt-service.yaml SERVICE_PORT: Port used to access the applicationZIGBEE_ADAPTER_PORT: Port used to allow access to an external zigbee controller 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: name: zigbee2mqtt-servicespec: type: LoadBalancer selector: app.kubernetes.io&#x2F;name: zigbee2mqtt ports: - name: http protocol: TCP port: SERVICE_PORT targetPort: SERVICE_PORT - name: adapter protocol: TCP port: ZIGBEE_ADAPTER_PORT targetPort: ZIGBEE_ADAPTER_PORT externalIPs: - A.B.C.D Firewallsudo ufw allow from A.B.C.0/24 to any port ZIGBEE_ADAPTER_PORT proto tcpsudo ufw allow from A.B.C.0/24 to any port SERVICE_PORT proto tcp Configurationhttps://www.reddit.com/r/homeassistant/comments/zxee4n/zigbee2mqtt_error_failed_to_connect_to_the/","categories":[],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://questionableengineering.com/tags/Kubernetes/"},{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Microk8s","slug":"Microk8s","permalink":"http://questionableengineering.com/tags/Microk8s/"}]},{"title":"Transmission Kubernetes Setup on Ubuntu","slug":"Transmission-Kubernetes-Setup-on-Ubuntu","date":"2023-08-23T16:28:03.000Z","updated":"2024-07-15T00:17:54.922Z","comments":true,"path":"2023/08/23/Transmission-Kubernetes-Setup-on-Ubuntu/","link":"","permalink":"http://questionableengineering.com/2023/08/23/Transmission-Kubernetes-Setup-on-Ubuntu/","excerpt":"","text":"Install the k8s cluster if not alreadly installed12microk8s enable dns storage helm3microk8s status Pull the helm chart12microk8s helm repo add truecharts https:&#x2F;&#x2F;charts.truecharts.org&#x2F;microk8s helm pull truecharts&#x2F;transmission Modify the values.yaml file to set the configuration123tar -xvf .&#x2F;transmission-*.tgzcd transmissioncat values.yamls Make the data directoryCONTAINER_ROOT_PATH 123mkdir -p CONTAINER_ROOT_PATHsudo chown -R 777 CONTAINER_ROOT_PATH Create Persistent Volumes and Persistent Volume ClaimsTransmission can think you deleted all your files if the harddisk containing your files fails to mount and the configuration files are on a different disk. Persistent VolumesConfigtransmission-config-pv.yaml HOSTNAME: Local computer hostname 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: transmission-config-pvspec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: CONTAINER_ROOT_PATH&#x2F;config # This must exist on the host nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io&#x2F;hostname operator: In values: - HOSTNAME microk8s kubectl apply -f transmission-config-pv 123microk8s kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtransmission-config-pv 100Gi RWO Retain Bound default&#x2F;transmission-config-pvc local-storage 9m44s Datatransmission-data-pv.yaml 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: transmission-data-pvspec: capacity: storage: 5Ti volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: CONTAINER_ROOT_PATH&#x2F;data # This must exist on the host nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io&#x2F;hostname operator: In values: - HOSTNAME microk8s kubectl apply -f transmission-data-pv.yaml microk8s kubectl get pv 123NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtransmission-config-pv 100Gi RWO Retain Bound default&#x2F;transmission-config-pvc local-storage 3h28mtransmission-data-pv 5Ti RWO Retain Bound default&#x2F;transmission-data-pvc local-storage 19s Persistent Volume ClaimsConfigtransmission-config-pvc.yamlBonds to transmission-config-pv 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: transmission-config-pvcspec: storageClassName: local-storage # Empty string must be explicitly set otherwise default StorageClass will be set accessModes: - ReadWriteOnce volumeName: transmission-config-pv resources: requests: storage: 100Gi microk8s kubectl apply -f transmission-config-pvc.yaml Datatransmission-data-pvc.yamlBonds to transmission-data-pvc 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: transmission-data-pvcspec: storageClassName: local-storage # Empty string must be explicitly set otherwise default StorageClass will be set accessModes: - ReadWriteOnce volumeName: transmission-data-pv resources: requests: storage: 5Ti microk8s kubectl apply -f transmission-data-pvc.yaml microk8s kubectl get pvc 123NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtransmission-config-pvc Bound transmission-config-pv 100Gi RWO local-storage 3h21mtransmission-data-pvc Bound transmission-data-pv 5Ti RWO local-storage 51s Pull the syncthing helm chart123microk8s helm repo add truecharts https:&#x2F;&#x2F;charts.truecharts.org&#x2F;microk8s helm repo updatemicrok8s helm pull truecharts&#x2F;transmission Modify the values.yaml file to set the configurationWe’re going to set the transmission configuration within the container. 123tar -xvf .&#x2F;transmission-*.tgzcd transmissionnano values.yamls 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128image: repository: tccr.io&#x2F;truecharts&#x2F;transmission pullPolicy: IfNotPresent tag: v4.0.3@sha256:245158e56dae5ca2da2cac5e9e85d4879685e8f302ed955ba144162d504307e4service: main: ports: main: port: WEBGUI_PORT_NUMBER torrent: enabled: true ports: torrent: enabled: true port: TRANSMISSION_PORT protocol: tcp torrentudp: enabled: true port: TRANSMISSION_PORT protocol: udpworkload: main: podSpec: containers: main: probes: liveness: enabled: true type: tcp startup: enabled: true type: tcp readiness: enabled: true type: tcp env: # PUID: 1001 # URL is set here so it wont be able to get overwritten by the user # as this will break the probes, if the need arises we can expose it. TRANSMISSION__RPC_URL: &quot;&#x2F;transmission&quot; TRANSMISSION__RPC_USERNAME: &quot;USERNAME&quot; TRANSMISSION__RPC_PASSWORD: &quot;RANDOM_PASSWORD&quot; TRANSMISSION__RPC_AUTHENTICATION_REQUIRED: true # TRANSMISSION__ALT_SPEED_DOWN: 50 # TRANSMISSION__ALT_SPEED_ENABLED: false # TRANSMISSION__ALT_SPEED_TIME_BEGIN: 540 # TRANSMISSION__ALT_SPEED_TIME_DAY: 127 # TRANSMISSION__ALT_SPEED_TIME_ENABLED: false # TRANSMISSION__ALT_SPEED_TIME_END: 1020 # TRANSMISSION__ALT_SPEED_UP: 50 # TRANSMISSION__BIND_ADDRESS_IPV4: &quot;0.0.0.0&quot; # TRANSMISSION__BIND_ADDRESS_IPV6: &quot;::&quot; # TRANSMISSION__BLOCKLIST_ENABLED: true # TRANSMISSION__BLOCKLIST_URL: &quot;https:&#x2F;&#x2F;github.com&#x2F;Naunter&#x2F;BT_BlockLists&#x2F;releases&#x2F;download&#x2F;v.1&#x2F;bt_blocklists.gz&quot; TRANSMISSION__CACHE_SIZE_MB: 4 # TRANSMISSION__DHT_ENABLED: true TRANSMISSION__DOWNLOAD_DIR: &quot;&#x2F;data&#x2F;Completed&quot; # TRANSMISSION__DOWNLOAD_QUEUE_ENABLED: true # TRANSMISSION__DOWNLOAD_QUEUE_SIZE: 5 # TRANSMISSION__ENCRYPTION: 1 # TRANSMISSION__IDLE_SEEDING_LIMIT: 30 # TRANSMISSION__IDLE_SEEDING_LIMIT_ENABLED: false TRANSMISSION__INCOMPLETE_DIR: &quot;&#x2F;data&#x2F;Incomplete&quot; # TRANSMISSION__INCOMPLETE_DIR_ENABLED: true # TRANSMISSION__LPD_ENABLED: false # TRANSMISSION__MESSAGE_LEVEL: 2 # TRANSMISSION__PEER_CONGESTION_ALGORITHM: &quot;&quot; # TRANSMISSION__PEER_ID_TTL_HOURS: 6 # TRANSMISSION__PEER_LIMIT_GLOBAL: 200 # TRANSMISSION__PEER_LIMIT_PER_TORRENT: 50 TRANSMISSION__PEER_PORT: &quot;&#123;&#123; .Values.service.torrent.ports.torrent.port &#125;&#125;&quot; # TRANSMISSION__PEER_PORT_RANDOM_HIGH: 65535 # TRANSMISSION__PEER_PORT_RANDOM_LOW: 49152 # TRANSMISSION__PEER_PORT_RANDOM_ON_START: false # TRANSMISSION__PEER_SOCKET_TOS: default&quot; # TRANSMISSION__PEX_ENABLED: true # TRANSMISSION__PORT_FORWARDING_ENABLED: false # TRANSMISSION__PREALLOCATION: 1 # TRANSMISSION__PREFETCH_ENABLED: true # TRANSMISSION__QUEUE_STALLED_ENABLED: true # TRANSMISSION__QUEUE_STALLED_MINUTES: 30 # TRANSMISSION__RATIO_LIMIT: 2 # TRANSMISSION__RATIO_LIMIT_ENABLED: false # TRANSMISSION__RENAME_PARTIAL_FILES: true # TRANSMISSION__RPC_BIND_ADDRESS: &quot;0.0.0.0&quot; # TRANSMISSION__RPC_ENABLED: true # TRANSMISSION__RPC_HOST_WHITELIST: &quot;&quot; # TRANSMISSION__RPC_HOST_WHITELIST_ENABLED: false TRANSMISSION__RPC_PORT: &quot;&#123;&#123; .Values.service.main.ports.main.port &#125;&#125;&quot; # TRANSMISSION__RPC_WHITELIST: &quot;&quot; # TRANSMISSION__RPC_WHITELIST_ENABLED: false # TRANSMISSION__SCRAPE_PAUSED_TORRENTS_ENABLED: true # TRANSMISSION__SCRIPT_TORRENT_DONE_ENABLED: false # TRANSMISSION__SCRIPT_TORRENT_DONE_FILENAME: &quot;&quot; # TRANSMISSION__SEED_QUEUE_ENABLED: false # TRANSMISSION__SEED_QUEUE_SIZE: 10 # TRANSMISSION__SPEED_LIMIT_DOWN: 100 # TRANSMISSION__SPEED_LIMIT_DOWN_ENABLED: false # TRANSMISSION__SPEED_LIMIT_UP: 100 # TRANSMISSION__SPEED_LIMIT_UP_ENABLED: false # TRANSMISSION__START_ADDED_TORRENTS: true # TRANSMISSION__TRASH_ORIGINAL_TORRENT_FILES: false # TRANSMISSION__UMASK: 2 # TRANSMISSION__UPLOAD_SLOTS_PER_TORRENT: 14 # TRANSMISSION__UTP_ENABLED: true # TRANSMISSION__WATCH_DIR: &quot;&#x2F;watch&quot; # TRANSMISSION__WATCH_DIR_ENABLED: falsepersistence: config: enabled: true mountPath: &quot;&#x2F;config&quot; type: pvc existingClaim: transmission-config-pvc data: enabled: true mountPath: &quot;&#x2F;data&quot; type: pvc existingClaim: transmission-data-pvcportal: open: enabled: truemanifestManager: enabled: false Install the helm chart with our values.yaml file1microk8s helm install syncthing-server truecharts&#x2F;syncthing --values .&#x2F;values.yaml Check the status of the installed application.12345microk8s statusmicrok8s kubectl describe podsmicrok8s kubectlmicrok8s kubectl logsmicrok8s kubectl describe pods Common ErrorsError from server (BadRequest): container in pod is waiting to start: ContainerCreatingYou probably need to change the permissions on the PV directory. This path is what is written in the PersistentVolume in the path variable. A quick chmod -R 777 to this path will most likely fix the issue. The conatiner should update the permissions once it runs. Service yaml fileWe need to expose the service to the outside world.Thankfully microk8s has a built in loadbalancer called metallb WEBGUI_PORT_NUMBER: The port you wish to run the webgui onTRANSMISSION_PORT: The port Transmission uses to comminicate with external parties. transmission-service.yaml 1234567891011121314151617181920212223apiVersion: v1kind: Servicemetadata: name: transmission-servicespec: type: LoadBalancer selector: app.kubernetes.io&#x2F;name: transmission ports: - name: transmission-tcp protocol: TCP port: TRANSMISSION_PORT targetPort: TRANSMISSION_PORT - name: transmission-udp protocol: UDP port: TRANSMISSION_PORT targetPort: TRANSMISSION_PORT - name: transmission-rpc-tcp protocol: TCP port: WEBGUI_PORT_NUMBER # web gui port targetPort: WEBGUI_PORT_NUMBER externalIPs: - A.B.C.D More information herehttps://medium.com/swlh/kubernetes-external-ip-service-type-5e5e9ad62fcd Apply the service1microk8s kubectl apply -f .&#x2F;transmission-service.yaml Confirm the service is active1microk8s kubectl get services transmission-service 12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtransmission-service LoadBalancer A.B.C.D X.Y.Z.A TRANSMISSION_PORT:TRANSMISSION_PORT_2&#x2F;TCP,TRANSMISSION_PORT:TRANSMISSION_PORT_2 &#x2F;UDP,WEBGUI_PORT_NUMBER:31056&#x2F;TCP 9h Firewall RulesThis assumes you are using ufw.ufw is bascally a wrapper for IPTABLES. If you have ever used IPTABLES before you understand why ufw exists.https://docs.syncthing.net/users/firewall.html 12345678sudo ufw default allow routed sudo ufw allow from A.B.C.0&#x2F;24 to any port WEBGUI_PORT_NUMBER proto tcpsudo ufw allow from E.F.G.H&#x2F;24 to any port TRANSMISSION_PORT proto tcpsudo ufw allow from E.F.G.H&#x2F;24 to any port TRANSMISSION_PORT proto udpsudo ufw status Referenceshttps://kubernetes.io/docs/concepts/services-networking/service/","categories":[],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://questionableengineering.com/tags/Kubernetes/"},{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Microk8s","slug":"Microk8s","permalink":"http://questionableengineering.com/tags/Microk8s/"}]},{"title":"MQTT K8s Setup on Ubuntu 20.04","slug":"MQTT-K8s-Setup-on-Ubuntu-20-04","date":"2023-04-16T13:35:18.000Z","updated":"2024-01-16T05:04:37.275Z","comments":true,"path":"2023/04/16/MQTT-K8s-Setup-on-Ubuntu-20-04/","link":"","permalink":"http://questionableengineering.com/2023/04/16/MQTT-K8s-Setup-on-Ubuntu-20-04/","excerpt":"","text":"Install the k8s cluster12microk8s enable dns storage helm3microk8s status Pull the Mqtt helm chart12microk8s helm repo add truecharts https:&#x2F;&#x2F;charts.truecharts.org&#x2F;microk8s helm pull truecharts&#x2F;mosquitto --version 8.0.11 Modify the values.yaml file to set the configurationWe’re going to set the mosquitto configuration within the container. 123tar -xvf .&#x2F;mosquitto-8.0.11.tgzcd mosquittocat values.yamls Bottom of values.yaml 123456789101112131415persistence: data: enabled: true mountPath: &quot;&#x2F;mosquitto&#x2F;data&quot; configinc: enabled: true mountPath: &quot;&#x2F;mosquitto&#x2F;configinc&quot; mosquitto-config: enabled: &quot;true&quot; mountPath: &quot;&#x2F;mosquitto&#x2F;config&#x2F;mosquitto.conf&quot; subPath: &quot;mosquitto.conf&quot; type: &quot;custom&quot; volumeSpec: configMap: name: &#39;&#123;&#123; template &quot;tc.common.names.fullname&quot; . &#125;&#125;-config&#39; mosquitto-config is a config map. configinc and data are both directories on the host machine. Make the data directory12345mkdir &#x2F;mnt&#x2F;Poseidon&#x2F;k8s&#x2F;mosquittomkdir &#x2F;mnt&#x2F;Poseidon&#x2F;k8s&#x2F;mosquitto&#x2F;datamkdir &#x2F;mnt&#x2F;Poseidon&#x2F;k8s&#x2F;mosquitto&#x2F;configincchown -R 777 &#x2F;mnt&#x2F;Poseidon&#x2F;k8s&#x2F;mosquitto Create Persistent Volumes and Persistent Volume ClaimsPersistent VolumesconfigincHOSTNAME: example_hostFILE_PATH: /path/to/files/ mosquitto-configinc-pv.yaml 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: mosquitto-configinc-pvspec: capacity: storage: 3Ti volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: &#x2F;path&#x2F;to&#x2F;files&#x2F;mosquitto&#x2F;configinc # This must exist on the host nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io&#x2F;hostname operator: In values: - example_host microk8s kubectl apply -f mosquitto-configinc-pv.yaml 1persistentvolume&#x2F;mosquitto-configinc-pv created DataHOSTNAME: example_host mosquitto-data-pv.yaml 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: mosquitto-data-pvspec: capacity: storage: 3Ti volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: &#x2F;path&#x2F;to&#x2F;files&#x2F;mosquitto&#x2F;data # This must exist on the host nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io&#x2F;hostname operator: In values: - example_host microk8s kubectl apply -f mosquitto-data-pv.yaml 1persistentvolume&#x2F;mosquitto-data-pv created Verify PVmicrok8s kubectl get pv 123NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmosquitto-configinc-pv 3Ti RWO Retain Available local-storage 97smosquitto-data-pv 3Ti RWO Retain Available local-storage 35s Persistent Volume Claimsconfigincmosquitto-configinc-pvc.yaml mosquitto-configinc-pvc.yamlBonds to mosquitto-configinc-pv 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mosquitto-configinc-pvcspec: storageClassName: local-storage # Empty string must be explicitly set otherwise default StorageClass will be set accessModes: - ReadWriteOnce volumeName: mosquitto-configinc-pv resources: requests: storage: 3Ti microk8s kubectl apply -f mosquitto-configinc-pvc.yaml Datamosquitto-data-pvc.yamlBonds to mosquitto-data-pv 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mosquitto-data-pvcspec: storageClassName: local-storage # Empty string must be explicitly set otherwise default StorageClass will be set accessModes: - ReadWriteOnce volumeName: mosquitto-data-pv resources: requests: storage: 3Ti microk8s kubectl apply -f mosquitto-data-pvc.yaml Verify PVmicrok8s kubectl get pvc 12mosquitto-configinc-pvc Bound mosquitto-configinc-pv 3Ti RWO local-storage 4m44smosquitto-data-pvc Bound mosquitto-data-pv 3Ti RWO local-storage 2m7s Update the values fileWe know need to tell the helm chart to mount our directories 1nano values.yamls Update the persistence section as follows 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677image: repository: tccr.io&#x2F;truecharts&#x2F;eclipse-mosquitto tag: 2.0.15@sha256:9e1fbb32ae27aaaf18432ff4e7e046c54fda8a630851dadf0c332a94e81fbf67 pullPolicy: IfNotPresentservice: main: ports: main: port: 1883 targetPort: 1883 websockets: enabled: true ports: websockets: enabled: true port: 9001 targetPort: 9001ingress: websockets: autoLink: trueauth: # -- By enabling this, &#96;allow_anonymous&#96; gets set to &#96;false&#96; in the mosquitto config. enabled: falsewebsockets: # -- By enabling this, an additional listener with protocol websockets is added in the mosquitto config. enabled: falseconfigmap: config: enabled: true data: mosquitto.conf: | listener &#123;&#123; .Values.service.main.ports.main.targetPort &#125;&#125; &#123;&#123;- if .Values.websockets.enabled &#125;&#125; listener &#123;&#123; .Values.service.websockets.ports.websockets.targetPort &#125;&#125; protocol websockets &#123;&#123;- end &#125;&#125; &#123;&#123;- if .Values.auth.enabled &#125;&#125; allow_anonymous false &#123;&#123;- else &#125;&#125; allow_anonymous true &#123;&#123;- end &#125;&#125; &#123;&#123;- if .Values.persistence.data.enabled &#125;&#125; persistence true persistence_location &#123;&#123; .Values.persistence.data.mountPath &#125;&#125; autosave_interval 1800 &#123;&#123;- end &#125;&#125; &#123;&#123;- if .Values.persistence.configinc.enabled &#125;&#125; include_dir &#123;&#123; .Values.persistence.configinc.mountPath &#125;&#125; &#123;&#123;- end &#125;&#125;persistence: data: enabled: true mountPath: &quot;&#x2F;mosquitto&#x2F;data&quot; type: pvc existingClaim: mosquitto-data-pvc configinc: enabled: true mountPath: &quot;&#x2F;mosquitto&#x2F;configinc&quot; type: pvc existingClaim: mosquitto-configinc-pvc mosquitto-config: enabled: &quot;true&quot; mountPath: &quot;&#x2F;mosquitto&#x2F;config&#x2F;mosquitto.conf&quot; subPath: &quot;mosquitto.conf&quot; type: &quot;custom&quot; volumeSpec: configMap: name: &#39;&#123;&#123; template &quot;tc.common.names.fullname&quot; . &#125;&#125;-config&#39;portal: enabled: false Install the helm chart1microk8s helm install mosquitto truecharts&#x2F;mosquitto --version 8.0.11 --values .&#x2F;values.yaml Check the status of the installed application.12345microk8s statusmicrok8s kubectl show podsmicrok8s kubectlmicrok8s kubectl logsmicrok8s kubectl describe pods Common ErrorsError from server (BadRequest): container in pod is waiting to start: ContainerCreatingYou probably need to change the permissions on the PV directory. This path is what is written in the PersistentVolume in the path varaibale. A quick chmod -R 777 to this path will most likely fix the issue. The conatiner should update the permissions once it runs. Service yaml fileWe need to expose the service to the outside world.Thankfully microk8s has a built in loadbalancer called metallb Replace Y with the MQTT port number. Default 1883Replace Z with the MQTT API number Default 9001 1234567891011121314151617181920apiVersion: v1kind: Servicemetadata: name: mqtt-servicespec: type: LoadBalancer selector: app.kubernetes.io&#x2F;name: mosquitto ports: - name: http protocol: TCP port: Y targetPort: Y - name: https protocol: TCP port: Z targetPort: Z externalIPs: - X.X.X.X More information herehttps://medium.com/swlh/kubernetes-external-ip-service-type-5e5e9ad62fcd Apply the service1microk8s kubectl apply -f .&#x2F;mqtt-service.yaml Confirm the service is active1microk8s kubectl describe services mqtt-service Local test clientYou can test the connection locally on the server with this simple CLI mqtt clientTest clienthttps://mqttx.app/cli Firewall RulesThis assumes you are using ufw.ufw is bascally a wrapper for IPTABLES. If you have ever used IPTABLES before you understand why ufw exists. 123sudo ufw default allow routed sudo ufw allow from X.X.X.0&#x2F;X to any port Y proto tcpsudo ufw status Test external ConnectionsOnce again try to connect to port Y with https://mqttx.app/cli Referenceshttps://kubernetes.io/docs/concepts/services-networking/service/","categories":[],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://questionableengineering.com/tags/Kubernetes/"},{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Microk8s","slug":"Microk8s","permalink":"http://questionableengineering.com/tags/Microk8s/"}]},{"title":"Microscope Camera","slug":"Microscope-Camera","date":"2023-02-20T15:21:18.000Z","updated":"2023-09-02T20:44:58.302Z","comments":true,"path":"2023/02/20/Microscope-Camera/","link":"","permalink":"http://questionableengineering.com/2023/02/20/Microscope-Camera/","excerpt":"","text":"Camera Selection5MP camera is the min. Cellphone adapter is also available but I want a dedicated camera. ResearchThis was a similar project to what I am trying to build. Very nice blog as wellhttps://www.briandorey.com/post/raspberry-pi-high-quality-camera-on-the-microscope HardwareRaspberrypi High Quaility Camera 12 MP C mounthttps://www.adafruit.com/product/4561 The framerate on a raspberry pi is very low so I perchased an arducam camera to usb board.https://www.arducam.com/product/arducam-uvc-camera-adapter-board-for-12mp-imx477-raspberry-pi-hq-camera/https://www.amazon.com/gp/product/B08NVG2CY4/ref=ppx_od_dt_b_asin_title_s00?ie=UTF8&amp;psc=1I got the idea to use the arducam adapter from https://www.briandorey.com/post/raspberry-pi-high-quality-camera-on-the-microscope User Manual for the Pi camera adapter.Arducam-UVC-Camera-Adapter-Board-for-12MP-IMX477-Raspberry-Pi-HQ-Camera-b0278.pdf UC-733_DIM.pdf Raspberrypi High Quaility Camera 12 MP C mount Components: Raspberrypi High Quaility Camera 12 MP C mount, ribbion cable, and Arducam UVC Camera Adapter Board. Connecting the Arducam UVC Camera Adapter Board to the Raspberrypi High Quaility Camera MechanicalCamera Adapter MountI originally went with mounting the camera without any lenses. Using this Raspberry Pi HQ camera adapter 3d printedhttps://www.printables.com/model/59927-raspberry-hq-camera-to-amscope-adapter The adapter mounted to the camera port of my AmScope T720B Scope Front-Side Side Using the above setup I was able to take this image with the camera.This is a plant leaf from the Codiaeum variegatum at ~40x Camera IssuesThe camera sensor size causes an inherent magnification effect called crop factor. The field of view becomes reduced due to the this magnification effect. You can see this in the image above.More information herehttps://learn.adafruit.com/raspberry-pi-hq-camera-lenses/crop-factorhttps://www.microscopeworld.com/p-3341-microscope-c-mount-field-of-view.aspxhttps://www.microscopeworld.com/t-microscope_c-mounts.aspx In order to correct for the inherent magnification effect an adapter was purchased.From these sources I was able to find that 0.3x to 0.5x is about correct for my sensor size Adapter lens selectionAmScope RU050 0.5X Reduction Lens for C-mount Cameras FMA050 Cmount to 23mm adapter. This fits my AmScope T720B Scope https://www.ebay.com/itm/141950389592 Picture of dust taken with the 0.5x adapter. It’s hard to tell from the images above but the field of view is noticably larger. Enclosure To protect the hardware I designed an enclouse Microscope_Camera_Enclosure-Lid.stl Microscope_Camera_Enclosure-Enclosure.stl Microscope_Camera_Enclosure.FCStd Microscope_Camera_Enclosure.FCStd1 SoftwareI had problems with Cheese constantly changing the exposure. Thankfully many have suffered from this issue before. I found this link that is specific to the raspberry pi but useful. https://hackernoon.com/polising-raspberry-pi-high-quality-camera-3z113u18I had to use a more advanced pieace of software called qv4l2 To install on debian linux sudo apt-get install qv4l2 Referenceshttps://www.edmundoptics.com/knowledge-center/application-notes/microscopy/understanding-microscopes-and-objectives/https://www.microscopeworld.com/p-3341-microscope-c-mount-field-of-view.aspxhttps://www.microscopeworld.com/t-microscope_c-mounts.aspxhttps://www.briandorey.com/post/raspberry-pi-high-quality-camera-on-the-microscope","categories":[],"tags":[]},{"title":"Microscope Selection","slug":"Microscope-Selection","date":"2023-02-20T15:15:30.000Z","updated":"2023-04-19T03:44:56.086Z","comments":true,"path":"2023/02/20/Microscope-Selection/","link":"","permalink":"http://questionableengineering.com/2023/02/20/Microscope-Selection/","excerpt":"","text":"Researchhttps://www.reddit.com/r/microbiology/comments/q5h8vv/what_is_considered_a_great_microscope_in_todays/https://www.edmundoptics.com/knowledge-center/application-notes/microscopy/understanding-microscopes-and-objectives/ RequirementsFrom the research above I was able to settle on the following requirements Greater than 1000X Kohler light source infinity corrected optics Camera port SelectionAmscope T720B https://amscope.com/products/t720b-hc2 Infinity Corrected Optical System with High ResolutionFully Coated Optics with Crystal Clear &amp; Sharp ImagesPrecise Mechanical Control SystemReversed Nosepiece DesignKohler Illumination System with Field Diaphragm for Lighting Control30-Degree Inclined, 360-Degree Swiveling, Compensation Free Trinocular HeadEight Magnification Levels: 40X, 80X, 100X, 200X, 400X, 800X, 1000X, 2000XIntensity-Variable Transmitted LED Lighting SystemAbbe Condenser with Iris Diaphragm and Filter HolderRack and Pinion Adjustment for CondenserLow Position Coaxial Stage Movement Controlling KnobsDual Side Coaxial Coarse and Fine Focusing ControlAdjustable Interpupillary DistanceAdjustable Diopter on EyepiecesDurable Cast Alloy Frame with Stain Resistant Enamel FinishFour Infinity Plan Objectives IncludedTwo Pairs of Extreme Widefield Eyepieces Included (EWF10X &amp; WF20X)Quadruple, Reversed, Extra-Large Nosepiece with Wide, Knurled Grip for Easy OperationLarge Double Layer Mechanical Stage with Stain Resistant CoatingUpward Stage Limit Stop to Protect Objectives and SlidesManufactured under ISO 9001 Quality Control StandardsExcellent Five (5) Year Factory Warranty Specifications :Optical System: infinity correctedNosepiece: reversed, ball bearing quadrupleHead: gemel type trinocular head, 30-degree inclinedEyepiece: high eye-point eyepieces, WF10X22mm, WF20XObjectives: infinity plan objective 4X, 10X, 40X (spring), 100X (spring, oil)Focusing: low position coaxial focus systemFocusing Range: 1-3/16” (30mm)Interpupillary Adjustment Range: 2-3/16” - 3” (55-75mm)Mechanical Tube Length: 6-5/16” (160mm)Mechanical Stage: 8.5” x 5.9” (216mm x 150mm)Stage Traveling Range: 2.9” x 2” (75x50mm)Focusing Rang: 0.95” (24mm)Division of Fine Focusing: 0.00003935” (0.001mm)Illuminator: Built-in Kohler LED illumination systemCondenser: N.A. 1.25 achromatic condenserIllumination: Kohler, LEDPower Supply: 90V-240 wide voltage, CE certifiedBuilt in measurement capabilitiesWeight: 28 lbs Packing List :One Trinocular Compensation-Free HeadOne Microscope Body with Frame, Base, and Kohler Illumination SystemFour High Quality DIN Plan Achromatic Objectives: 4X, 10X, 40X and 100XOne Pair of Widefield Eyepieces: WF10XOne Pair of Widefield Eyepieces: WF20XOne Dust CoverOne HDMI cameraOne HDMI cableImmersion OilUser’s Manual PurchasingThere is no reason pay top dollar for a microscope. You can find massive discounts on refunbised microscopes on ebay!https://www.ebay.com/itm/381546384234?hash=item58d5efcb6a:g:9qkAAOSwPXFcFU2K Referenceshttps://storage.googleapis.com/software-download-d79bb.appspot.com/Manual%20Download%20Files/Manual%20Files/Compound/720%20Series_Manual_151006.pdf","categories":[],"tags":[]},{"title":"WLED Degchi Lamp","slug":"WLED-Degchi-Lamp","date":"2022-08-17T20:16:59.000Z","updated":"2023-09-03T12:43:01.366Z","comments":true,"path":"2022/08/17/WLED-Degchi-Lamp/","link":"","permalink":"http://questionableengineering.com/2022/08/17/WLED-Degchi-Lamp/","excerpt":"","text":"While traveling in India I came across these nice looking tin lamps. It is sometimes called a Degchi lamp. Since I am not a fan of open flames so leds will have to fill in the role of a candle. This build will be a bit rough. I will refine the lamp in later posts. PartsControllerESP8266 Esp8266 Huzzah Esp8266 Huzzah documentation Level shifter GeeekPi 6Pack TXS0108E 8 Channel Logic Level Converter Bi-Directional High Speed Full Duplex Shifter 3.3V 5V for Arduino Raspberry Pi LED Strip Led Strip Power supply 5v 10 amp power upply with barrel plug Plug Barrel plug SoftwareWLEDwled BinaryWled binary FlashingYou will need a 3.3v usb to serial adapter. An FTDI based usb to serial adapteris perfered. https://docs.espressif.com/projects/esptool/en/latest/esp32/Method 2 https://kno.wled.ge/basics/install-binary/ Connections Connection between the usb to serial adapter and the ESP8266. Logic Level set to 3.3V ESP -&gt; FTDI TX -&gt; RX RX -&gt; TX VCC -&gt; 5V GND -&gt; GND Operating system permission workaroundsIn order to write data to ttyUSB or other serieal ports you must be a member of the dialout group. sudo usermod -a -G dialout your_user_name Log out log in Flash WLED onto the ESPsudo python3 ./esptool.py -p /dev/ttyUSB0 write_flash 0x0 ./WLED_0.13.1_ESP8266.bin ElectronicsSchematic Electronics Rough FitPlaying around with the electronics to confirm that the design works as expected. Everything is hooked up but the lights are not working. Discovered that the OE pin on the TXS0108E needs to be pulled high to VA (ESP Logic Level High 3.3V) Physical ConstructionLampTest FitLets see how everything could fit inside the lamp. Who cares how it looks at the moment. We will polish it later. Electronics EnclosureFreeCad ModelDegchi_Lamp_Enclosure.FCStd First atttempt to create a quick enclosure. View of the enclosure lid. Enclosure STLsDegchi_Lamp_Enclosure-Degchi_Lamp_Electronics_Enclosure.stl Enclosure Main Body Degchi_Lamp_Enclosure-Degchi_Lamp_Electronics_Cover.stl Enclosure Main Body Enclosure Assembly 1 amp Fuse All the electronics seem to fit. A more professional version will be created when I get the parts. Forcing all the electronics into the enclosure. Everything is coming together. Power on testing Led MountsThe inside of the lamp is coated in a thick non conductive coating. For the time being the led strip is just placed inside the lamp body. Connecting to WIFIWLED 13.1 has some trouble connecting to wifi networks. Set wifi control channel to a fixed channel. e.g 1 Change channel width to 20 Mhz Bind to static ip in router Add the same static ip in wled wifi settings Results I am not a fan of this very large black power cable. I will replace the power cord with USB-C.After measuring the power usage of the lamp at peak load it looks like USB-C is a good option. Peak load 0.6 AmpsThis will be covered in a follow up article.","categories":[],"tags":[{"name":"Electronics","slug":"Electronics","permalink":"http://questionableengineering.com/tags/Electronics/"},{"name":"LED","slug":"LED","permalink":"http://questionableengineering.com/tags/LED/"},{"name":"WLED","slug":"WLED","permalink":"http://questionableengineering.com/tags/WLED/"},{"name":"Hyperion","slug":"Hyperion","permalink":"http://questionableengineering.com/tags/Hyperion/"},{"name":"RestAPI","slug":"RestAPI","permalink":"http://questionableengineering.com/tags/RestAPI/"}]},{"title":"TP Link U3T Ubuntu","slug":"TP-Link-U3T-Ubuntu","date":"2021-03-11T10:10:00.000Z","updated":"2022-03-28T23:32:26.245Z","comments":true,"path":"2021/03/11/TP-Link-U3T-Ubuntu/","link":"","permalink":"http://questionableengineering.com/2021/03/11/TP-Link-U3T-Ubuntu/","excerpt":"","text":"This device has the rtl8812bu chipset and you willneed to do a little more work to get it working.Thankfully there is a working driver available for it here: https://github.com/cilynx/rtl88x2bu To get it working, you will need to first install some packages and check out the Git repo: sudo apt-get install build-essential dkms git git clone https://github.com/cilynx/rtl88x2bu.git Then follow the instructions here to install the driver: cd rtl88x2bu VER=$(sed -n &#39;s/\\PACKAGE_VERSION=&quot;\\(.*\\)&quot;/\\1/p&#39; dkms.conf) sudo rsync -rvhP ./ /usr/src/rtl88x2bu-$&#123;VER&#125; sudo dkms add -m rtl88x2bu -v $&#123;VER&#125; sudo dkms build -m rtl88x2bu -v $&#123;VER&#125; sudo dkms install -m rtl88x2bu -v $&#123;VER&#125; sudo modprobe 88x2bu ReferencesAsk Ubuntu","categories":[],"tags":[{"name":"Fixes","slug":"Fixes","permalink":"http://questionableengineering.com/tags/Fixes/"}]},{"title":"Rigol DS1052E Oscilloscope Encoder Repair","slug":"Rigol-DS1052E-Oscilloscope-Encoder-Repair","date":"2020-10-19T02:42:26.000Z","updated":"2023-09-02T20:42:55.804Z","comments":true,"path":"2020/10/18/Rigol-DS1052E-Oscilloscope-Encoder-Repair/","link":"","permalink":"http://questionableengineering.com/2020/10/18/Rigol-DS1052E-Oscilloscope-Encoder-Repair/","excerpt":"","text":"Broken encoder I dropped my trusty Rigol scope off of the table while testing. The trigger encoder knob broke off. Replacement part After a bit of googling I was able to locate the part number. This is a very popular scope with hobbyist so this information was not all that hard to find. Opening the case The screws that hold the case on are Trox or star drive. There are 6 screws. Two screws on the bottom near the feet, two screws under the handle, and two screws on either side of the power socket. WARNING: Do not forget to remove the power button. If you try the case with the power button still in place the switch will snap off. The power button can be removed by pulling it upwards. You will need an extension bit to get at the screws under the handle Do not forget about the screws on the side Back case removed Once the case is removed, unscrew the standoffs on either side of the serial interface (DB9).Lift off the metal rf sheild You will need to remove all the screws inside of the case. All The power supply board must be removedDisconnect the power supply board. Watch out for the LCD lamp power cable (Red/White cable with JST connector) You will need to disconnect the white ribbion cable from the board at the bottom of the unit. The front case panel can now be removed. Power supply board. Power switch Front case panel removed. Picture of the 3 screws holding on the user control board.These will need to be removed. Replacing the encoder Boken encoder next to replacement encoder. Bottom of the user control board. Unsoldering required.WARNING: Rigol uses lead free solder. Only use lead free solder. If you mix leaded solder and lead free solder a new alloy with a higher melting point will be formed. Good luck removing that! Desoldered encoder Replacement encoder Put everything back together","categories":[],"tags":[{"name":"Electrical","slug":"Electrical","permalink":"http://questionableengineering.com/tags/Electrical/"},{"name":"Fixes","slug":"Fixes","permalink":"http://questionableengineering.com/tags/Fixes/"}]},{"title":"CNC Enclosure Door","slug":"CNC-Enclosure-Door","date":"2019-12-26T23:50:50.000Z","updated":"2023-09-04T22:45:23.928Z","comments":true,"path":"2019/12/26/CNC-Enclosure-Door/","link":"","permalink":"http://questionableengineering.com/2019/12/26/CNC-Enclosure-Door/","excerpt":"","text":"PartsMagnetic Door LatchDoorMagnetLatchMount.scad DoorMagnetLatchMount.stl 3d Printed Magnetic Latch mount Mounting the DoorHingesparametric_butt_hinge_3.7.scad parametric_butt_hinge_7.stl 3d Printed Hinges Door temporarily held on by clamps. Hinges mounted to the frame. Using a center locating punch through the hing mounting holes to locate where to drill the hole. Determining the correct drilling points with a punch Drilling Hinge Mounting Holes.In this case I tapped the acrylic plate with a 1/4-20 tap. These holes need to be kept away from the edge otherwise breakage can occur. Additionally, spreading the load across many holes reduces the stress on any one hole. Mounting the door with the hingesDoor mounted with the 1/4-20 bolts Door Mounted to Hinges Door HandleENCLOSURE_HANDLE.zip MU_HANDLE.3mf","categories":[],"tags":[{"name":"CNC","slug":"CNC","permalink":"http://questionableengineering.com/tags/CNC/"}]},{"title":"Grub Boot Loader Not Found","slug":"Grub-Boot-Loader-Not-Found","date":"2019-12-23T19:41:32.000Z","updated":"2019-12-23T19:55:14.000Z","comments":true,"path":"2019/12/23/Grub-Boot-Loader-Not-Found/","link":"","permalink":"http://questionableengineering.com/2019/12/23/Grub-Boot-Loader-Not-Found/","excerpt":"","text":"Insert Grub console picture here run the following commands ls Will show you all the drive parations (hd0,gpt1) ls (hdo,gpt1)/ Will give you a listing of all the files on the dive Find the drive that contains /boot (hd0,gpt1)/boot locate the grub.cfg configfile /PathToFile/grub.cfg configfile (hd0,gpt1)/boot/grub/grub.cfg Grub boot menu should start. Kernel is now running Fix any broken packagesdpkg –configure -a Check systemctl Look for any errors that may have occured. Fix any filesystem errors that may have occures look in /dev/disk/by-uuid Perform fsck on any drives that require a repair.fsck /dev/disk/by-uuid/abc456 Often the grub loader cannot find the efi file sudo apt install grub-efi-amd64","categories":[],"tags":[{"name":"Fixes","slug":"Fixes","permalink":"http://questionableengineering.com/tags/Fixes/"}]},{"title":"Numpy LeNet 5 with ADAM","slug":"Numpy-LeNet-5-with-ADAM","date":"2019-05-10T14:15:12.000Z","updated":"2023-09-03T12:42:00.526Z","comments":true,"path":"2019/05/10/Numpy-LeNet-5-with-ADAM/","link":"","permalink":"http://questionableengineering.com/2019/05/10/Numpy-LeNet-5-with-ADAM/","excerpt":"","text":"John W Grun AbstractIn this paper, a manually implemented LeNet-5 convolutional NN with an Adam optimizer written in Numpy will be presented. This paper will also cover a description of the data used to train and test the network,technical details of the implementation, the methodology of training the network and determining hyper parameters, and present the results of the effort. IntroductionLeNet-5 was created by Yuan Lecun and described in the paper “Gradient-Based Learning Applied To Document Recognition” . LeNet-5 was one of the first convolutional neural networks used on a large scale to automatically classify hand-written digits on bank checks in the United States. Prior to LeNet, most character recognition was done by using feature engineering by hand, followed by a simple machine learning model like K nearest neighbors (KNN) or Support Vector Machines (SVM). LeNet made hand engineering features redundant, because the network learns the best internal representation from training images automatically. This paper will cover some of the technical details of a manual Numpy implementation of LeNet-5 convolutional Neural Network including the details about the training set, structure of the lenet-5 CNN, weights and biases initialization, optimizer, gradient descent, the loss function, and speed enhancements. The paper will also cover the methodology used during training and selecting hyperparameters as well as the performance on the test dataset. Related workThere are numerous examples of numpy implementations of LeNet 5 found across the internet but, none with more significance than any other. Lenet-5 is now a common architecture used to teach new students fundamental concepts of convolutional neural network Data Description The MNIST database of handwritten digits, contains a training set of 60,000 examples, and a test set of 10,000 examples. Each example is a 28 x 28 pixel grayscale image.All training and test examples of the MNIST were converted from gray scale images to bilevel representation to simplify the function the CNN needed to learn. Only pixel positional information is required to correctly classify digits, while grayscale offers no useful additional information and only aids in increasing complexity. The labels of both the test and training examples were converted to one hot vectors to make them compatible with the softmax output and cross entropy loss function. Both indexes of the training and test sets were further randomized to ensure each batch was a random distribution of all 10 classes. Model DescriptionStructure The model is a implementation of LeNet 5 with the following structure: Input 28 x 28 Convolutional layer (Pad = 2 , Stride = 1, Activation = ReLu, Filters = 6, Size = 5) Max Pool (Filter = 2, Stride = 2) Convolutional layer (Pad = 0 , Stride = 1, Activation = ReLu, Filters = 16 ) Max Pool (Filter = 2, Stride = 2) Convolutional layer (Pad = 0 , Stride = 1, Activation = ReLu, Filters = 120) Fully Connected ( Size = 120, Activation = ReLu) Fully Connected (Size = 84, Activation = ReLu) Soft Max ( 10 Classes ) Weight and bias initializationSince the original lenet-5 predates many of the more optimal weight initialization schemes such as Xavier or HE initialization, the weights were initialized with numpy random.randn while biases were zero filled with numpy zeros. OptimizerAt first a constant learning rate optimizer was used for this network but, stable convergence required a very small learning rate. This small learning rate required a very long training time to achieve a reasonable accuracy on the test set. The constant learning rate optimizer was replaced with a numpy implementation of the ADAM optimizer. ADAM allowed for the use of higher learning rate that resulted in quicker and smoother convergence. The formulas that describe ADAM are shown below: Gradient DescentThis implementation of LeNet-5 uses Mini-batch gradient descent. Mini-batch gradient descent is a trade-off between stochastic gradient descent (training on 1 sample at a time) and gradient descent (training on the entire training set). In mini-batch gradient descent, the cost function (and therefore gradient) is averaged over a small number of samples. Mini batch gradient descent was selected due to its increased convergence rate and the ability to escape local minimum. Loss functionLeNet 5 produces a 10 class categorical output representing the numbers 0 to 9. The original LeNEt-5 used Maximum a posteriori (MAP) as the loss loss function. Cross-entropy was chosen as the loss function in this implementation instead of MAP since cross entropy appears to be the dominant loss function for similar classification problems and source code was available to check against. The formula for cross entropy loss is given below: Speed EnhancementsTo train the CNN in a reasonable amount of time several performance enhancements had to be made. The python profiler was used to identify locations in the code that would have the largest effect on performance. The convolutional and max pooling layers consumed the majority of the running time. The running time of the convolutional and max pool layers was decreased by first converting the single threaded functions into multithreaded functions. Processing was divided up equally across the number of threads. Once threading was confirmed to be working properly, the Numba Just in Time compiler (JIT) was employed to convert python functions into native code. Numba JIT was then liberally applied throughout the code. These enhancements reduced the training time from over 1 day to a few hours, constituting a 6-8x speed up on average. Method Description And Experimental ProcedureThe LeNet 5 model implementation was trained on the MNIST dataset. After each training, the training loss versus epoch was plotted. The learning rate was decreased until the training loss vs epochs was a monotonically decreasing function. The number of epochs was selected to minimize the training loss while the training loss continued to decrease with every training epoch. Adjustments to the epochs sometimes also required adjustments to the learning rate to keep the training loss vs epoch a monotonically decreasing function.In addition to the training loss, the prediction accuracy was computed. The accuracy was computed by the following method:The input images were forward propagated through the network with the weights and biases learned during training. The class with the largest magnitude was selected as the prediction. The predicted class was compared to the label for a given input image. The percentage of correct predictions was computed across all input images forward propagated through the network.The prediction accuracy was computed for both the training and testing sets . In a well trained network (one not underfitting or overfitting ) the test prediction accuracy should be close to the training prediction accuracy. If the training prediction accuracy is far greater than the test prediction accuracy it is a sign the network is overfitting on the training data and failing to generalize well.The batch size was selected primary upon the cache limitations of the processor. A batch size of around 32 was determined to be small enough to fit in cache while also large enough to reduce overhead from thread context switching. ResultsHyper parametersThe hyper parameters for this numpy implementation of LeNet 5 are as follows: Epochs = 20 Learning rate = 0.0002 Batch = 32 Training timeThe total training time was brought down from 26 hours to train on the entire training set of 60000 examples to only 2.75 hours after applying speed enhancements. Training loss The training loss of LeNet-5 as plotted over 20 epochs. The training loss is monotonically decreasing indicating the network is effectively learning to differentiate between the ten classes in the MNIST dataset. AccuracyAccuracy on test set = 95.07%Accuracy on Train set = 94.90%The Lenet-5 implementation achieved a high accuracy on the test and train sets without a significant difference in prediction accuracy between the train and test sets which would be an indication of overfitting. ConclusionA Lenet 5 Convolutional Neural Network has been implemented only using Numpy that yields prediction accuracies over 95% on the test set. The network was trained on all 60000 examples found in the MNIST dataset and tested against the 10000 examples in the MNIST test set. The network used the standard LeNet Architecture with modifications where required. To decrease convergence time, a numpy ADAM optimizer was written. Several speed enhancements such as multi threading and just in time compilation were employed to decrease training time to a reasonable period. References[1] Lavorini, Vincenzo. “Speeding up Your Code (4): in-Time Compilation with Numba.” Medium, Medium, 6 Mar. 2018, medium.com/@vincenzo.lavorini/speeding-up-your-code-4-in-time-compilation-with-numba-177d6849820e.[2] “Convolutional Neural Networks.” Coursera, www.coursera.org/learn/convolutional-neural-networks.[3] LeCun, Yann. MNIST Demos on Yann LeCun’s Website, yann.lecun.com/exdb/lenet/.[4] Lecun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. doi:10.1109/5.726791[5] “MNIST Database.” Wikipedia, Wikimedia Foundation, 11 Apr. 2019, en.wikipedia.org/wiki/MNIST_database.[6] “Cross Entropy.” Wikipedia, Wikimedia Foundation, 8 May 2019, en.wikipedia.org/wiki/Cross_entropy.[7] “Stochastic Gradient Descent.” Wikipedia, Wikimedia Foundation, 29 Mar. 2019, en.wikipedia.org/wiki/Stochastic_gradient_descent.","categories":[],"tags":[{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Research","slug":"Research","permalink":"http://questionableengineering.com/tags/Research/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://questionableengineering.com/tags/Machine-Learning/"}]},{"title":"QuadCopterBumbleBee","slug":"QuadCopterBumbleBee","date":"2019-01-20T16:08:15.000Z","updated":"2023-09-02T20:41:53.352Z","comments":true,"path":"2019/01/20/QuadCopterBumbleBee/","link":"","permalink":"http://questionableengineering.com/2019/01/20/QuadCopterBumbleBee/","excerpt":"","text":"BumbleBee Quad CopterFrameBumbleBeeFrame ConfigurationQuad X ControllerReadytosky Pixhawk PX4 Flight Controller Autopilot PIX 2.4.8 32 Bit Flight Control Board+Safety Switch+Buzzer+I2C Splitter Expand Module+16GB SD Card NavigationReadytosky M8N GPS Module Built-in Compass Protective Case with GPS Antenna Mount for Standard Pixhawk 2.4.6 2.4.8 Flight Controller Remote ControlTurnigy TGY-I6PWM To PPM Conversionusmile PPM Encoder With 10pin Input &amp; 4pin Output Cable For Pixhawk/PPZ/MK/MWC/Pirate Flight Control Motor ControllersTurnigy MultiStar V.20Internal BEC provides 5V to the rest of the systemWARNING: If using ESC BECs to power your system you may need to disconnect all but one of the 5 volt connections from the ESC BEC. Only 1 power source! Power3300 mAH Battery SoftwareFirmwareArdupilothttp://ardupilot.org/ Mission Planner ( Ground Control )APM Planner V2.0http://ardupilot.org/planner2/ PicturesFrameTesting Motor ConnectionsFront Left Front Right Rear Left Rear Right","categories":[],"tags":[]},{"title":"React_Native_Watch_Limit_Fix","slug":"React-Native-Watch-Limit-Fix","date":"2017-12-31T00:02:42.000Z","updated":"2017-12-31T00:08:59.000Z","comments":true,"path":"2017/12/30/React-Native-Watch-Limit-Fix/","link":"","permalink":"http://questionableengineering.com/2017/12/30/React-Native-Watch-Limit-Fix/","excerpt":"","text":"Ubuntu Watch filesystem limit Work AroundIn the terminal run sudo gedit /etc/sysctl.conf fs.inotify.max_user_instances=524288 fs.inotify.max_user_watches=524288 fs.inotify.max_queued_events=524288 Reboot for changes to take effect.","categories":[],"tags":[{"name":"Fixes","slug":"Fixes","permalink":"http://questionableengineering.com/tags/Fixes/"},{"name":"React Native","slug":"React-Native","permalink":"http://questionableengineering.com/tags/React-Native/"}]},{"title":"Electrical_Enclosures_With_OpenScad","slug":"Electrical-Enclosures-With-OpenScad","date":"2017-12-30T18:38:54.000Z","updated":"2020-09-04T03:38:43.975Z","comments":true,"path":"2017/12/30/Electrical-Enclosures-With-OpenScad/","link":"","permalink":"http://questionableengineering.com/2017/12/30/Electrical-Enclosures-With-OpenScad/","excerpt":"Using open scad to produce backplates for electrical enclosuresI needed a backplate for an electrical enclosure. Instead of waiting a few days I decided to 3d print one. Figure 1: NEMA4X Electrical Enclosure","text":"Using open scad to produce backplates for electrical enclosuresI needed a backplate for an electrical enclosure. Instead of waiting a few days I decided to 3d print one. Figure 1: NEMA4X Electrical Enclosure Tools: * Micrometer * Calculator * OpenScad * Cura OpenScad code: echo(version=version()); difference() &#123; color(&quot;red&quot;) translate([0, -0, 0]) linear_extrude(height = 2) square([121, 121], center = true); translate([-60.5,-60.5,0]) linear_extrude(height = 2) square([30,30], center = true); translate([60.5,-60.5,0]) linear_extrude(height = 2) square([30,30], center = true); translate([60.5,60.5,0]) linear_extrude(height = 2) square([30,30], center = true); translate([-60.5,60.5,0]) linear_extrude(height = 2) square([30,30], center = true); &#125; Figure 2: OpenScad Figure 3: Cura","categories":[],"tags":[{"name":"Electrical","slug":"Electrical","permalink":"http://questionableengineering.com/tags/Electrical/"},{"name":"OpenScad","slug":"OpenScad","permalink":"http://questionableengineering.com/tags/OpenScad/"},{"name":"Mechanical","slug":"Mechanical","permalink":"http://questionableengineering.com/tags/Mechanical/"}]},{"title":"Peer to Peer Domain Name System (P2PN-DNS)","slug":"Peer-to-Peer-Domain-Name-System-P2PN-DNS","date":"2017-12-29T19:49:34.000Z","updated":"2023-08-30T03:51:31.191Z","comments":true,"path":"2017/12/29/Peer-to-Peer-Domain-Name-System-P2PN-DNS/","link":"","permalink":"http://questionableengineering.com/2017/12/29/Peer-to-Peer-Domain-Name-System-P2PN-DNS/","excerpt":"","text":"John Grun AbstractIn this paper, we introduce the Peer to Peer Network Domain Name System (P2PN-DNS) a distributed implementation of theDomain Name System (DNS) that hopes to offer solutions to shortcomings of the current DNS such as susceptibility to outages while mitigating attempts of domain name censorship, and provide a fault tolerant name lookup service for software containers that is independent of upstream servers. Performance metrics including the time to reach consistency between nodes, service availability in lieu of loss of nodes, and average response time to DNS queries will be examined. I. IntroductionThe Domain Name System (DNS) is a critical element of internet infrastructure that associates IP addresses to human readable domain names. E.g Google.com is located a IP:172.217.6.238. When DNS was introduced in 1987 with the RFC 1034 /RFC 1035 17/18 standards, the internet consisted of relatively few computers where a simple hierarchical tree model functioned well.As the internet scaled, DNS was expanded to accommodate the exponential growth in computers. In this regard, DNS has been a tremendous success but, it is not without serious flaws, most notably respectability to attacks on or failures of the Root DNS servers. A Root DNS Server acts as the authoritative source for the entire network. Figure 1: DNS tree structure 24 In part due to the tree structure of DNS, there are only thirteen DNS Root Servers in the world. An attacker only needs to target a few DNS root servers to cripple IP address to domain name translation in an entire region effectively blocking internet access for most users. Attacks against DNS Root Servers have occurred and appear to be becoming more frequent from various entities, such as hackers, or state actors. It is also quite conceivable that DNS would be a target in wartime. Additionally, centrally located servers are vulnerable to regional events such as power outages or natural disasters. Another issue that has been seen in the wild is the censorship of websites by state, corporate, or other actors via DNS poisoning. DNS poisoning blocks access to websites by disrupting the domain name lookup between a DNS resolver and the regional DNS server. E.g Google.com is located at IP:172.217.6.238. China 26 and Iran 25 have both used DNS poisoning to block access to internet domains. Corporations such as Charter, Comcast, and Optonline 3 have used DNS poisoning to reroute search results away from competitor websites or to collect advertising profits. An additional limitation of the existing DNS system is a lack of software container support.Existing DNS solutions for software containers either rely upon a DNS repeater or upon a standard DNS server. These methods simply extend the existing DNS system. If the upstream DNS is made inoperable or poisoned, the containers will be affected as well. This dependency upon existing DNS introduces a single point of failure into what would otherwise be fault tolerant architecture. The aforementioned problems encountered are a consequence of the centralized hierarchical architecture of DNS. A different architecture can be defined to address these problems while providing the same functionality. We propose to build a decentralized Peer to Peer Network DNS (P2PN-DNS) that will not rely on central servers. We intended to take the best practices from existing distributed robust protocols such as bitTorrent and bitcoin to accomplish our goal. Also, each P2PN-DNS node should be able to contain an internal store of the domain name IP address relations without relying on additional software or servers. II. Related and Previous WorkThere have been several different methods over the years to address some or all of the aforementioned problems with DNS. Amazon Web Services (AWS) offers a implementation of DNS known as Amazon Route 53. Route 53 is believed to be a distributed DNS system but, architectural details have not been forthcoming from Amazon.27 Another project DC/OS, (the Distributed Cloud Operating System) contains an internal DNS repeater. While the DC/OS DNS repeater is distributed on the internal network and consists of multiple nodes, it is not a true distributed peer to peer DNS. The DC/OS DNS is a redundant DNS system that acts as a DNS forwarder from a DNS server further up the DNS tree making it vulnerable to the same issues as DNS.28 See https://dcos.io/ A third related work, Kad Node claims to be a small peer to peer DNS resolver or to act as a DNS proxy. Kad node stores DNS records in a distributed hash table, similar to our proposal but, it differs in that it does not appear to utilize a hash based DNS update ordering scheme. At the writing of this paper Kad Node appears to be inactive. 29 See Kad Node at https://github.com/kadtools/kad III. ContributionTechnical ProblemsThe main technical concerns affecting a distributed DNS are the same as those encountered in any system where information is distributed between disparate nodes, namely data consistency, availability, and network partition tolerance. As stated by the CAP (Consistency, Availability, and Partition Tolerance) theorem, it is impossible for a distributed data store to simultaneously provide all three of the guarantees of consistency, availability, and partition tolerance at any given time. This implies compromises must be made between the three. In the case of a distributed DNS the decision as to which guarantees to support and which to neglect is relatively straight forward. In a real world environment, computers crash, network connections fail, and infrastructure can be disabled, thus network partition tolerance is a requirement of any realistic distributed DNS. As a critical service of internet infrastructure, DNS must be always available for the internet to operate properly. Thus availability is a required guarantee as well. With partition tolerance and availability as required guarantees, continuous consistency becomes impossible to guarantee according to the CAP theorem. In the case of DNS, continuous consistency is not a requirement and eventual consistency is more than sufficient. In fact, currently a DNS record updates are eventually consistent and can take up to 24 hours to complete. Partition tolerance and availability can both be addressed by designing each node to operate independently of all the others on the network. The independent nodes need only exchange update information to ensure eventual consistency of the DNS records. In this architecture, the more nodes added to a network, the more reliable the system will become. In a distributed system relying upon eventual consistency to make data agree across nodes, the issue of order of updates arises, since there is the possibility of updates arriving at other nodes in a different order than they must be applied to ensure agreement between nodes.This can be addressed by writing updates to a commit log prior to writing the DNS update. A commit log is a data structure that keeps track of the operations to be performed in first in first out (FIFO) order. When a new DNS record update is received by a node, the update is first written to the commit log. While changes are in the commit log, actions can be taken to ensure the correct ordering of the updates. The ordering of the commits can be verified and corrected by comparing timestamps or by using a hashing based scheme. An update will only be removed from the commit log once the node has finished updating the DNS record. Figure 2: Commit Log Functionality 16 There are some properties of DNS record updates that allow for some simplifications.Since the update history of each domain name in DNS is independent of all other domain names, we do not need to ensure ordering between different domain names. This property can be utilized to simplify the consistency requirement since it greatly limits the amount of possible combinations in ordering. We need only establish an update chain on a per domain name basis.The update history can be established with timestamping. Each update is time stamped, and if every node is synced to the same clock the order of updates can be ordered correctly. If the nodes do not have a common clock source this method fails to guarantee the correct update ordering. This ordering problem is what data structures and algorithms such as Merkle trees and blockchains were developed to solve. A Blockchain or Merkle tree takes a hash of a data record, then the next data update in line is a hash of the new update combined with the previous hash. The update received by a node can be confirmed ordered correctly by hashing the previous hash with the current update and comparing the result. If the hash is the same, the DNS record can be updated. If the hash is incorrect a Quorum can be called to reach a consensus between nodes. Since a record update is only concerned about the most recent update and is not dependent upon the entire history of the update chain the ordering requirement can be relaxed to only include the last few updates. In this case, a Quorum can serve the purpose of correcting out of order entries. This situation may arise if some nodes do not receive a DNS update or network issues cause updates to arrive in the wrong order. The next technical challenge is how to store the DNS records. Since DNS are retrieved based upon the domain name, the most logical method is a simple key value store, where the domain name is used as the key and the corresponding ip address is the value. Another issue that arises in the real world is the need to automatically find peers. In a small scale system, manually assigning peer address is possible but, as the system scales this method quickly becomes untenable. Nodes must employ a method to locate each other without the intervention of humans. One such method that has proven to be quite effective in other distributed applications is the distributed hash table (DHT). A distributed hash table (DHT) provides a lookup service similar to a hash table: (key, value) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption. This allows a DHT to scale to an extremely large numbers of nodes while handling continual node arrivals, and departures. Figure 3: Distributed Hash Table Functionality 15 Possibly the hardest technical problem to address in a distributed system is known as the “Trust Problem.” The trust problem brings forth the question of which node is friendly, and which is malicious. If this implementation was utilized on the open web, the trust problem can be addressed with strategies such as proof of work as employed by technologies such as Blockchain. Nevertheless, if only running on a isolated network, methods such as trusted tokens would are viable. This problem is beyond the scope of this paper but, the authors felt it was important enough to mention and it will have to be addressed in future releases. IV. Algorithm and ImplementationThe implementation of P2PN-DNS involved the union of several aforementioned key concepts in distributed computing, the key-value store, distributed hash table, and the commit log. Figure 4: Implementation Block Diagram At the most basic level DNS can be viewed as a key value store where the domain name serves as the key and the IP address serves as the value. In practice, the DNS domain name to ip address relation is called a resource record and may contain a plethora of data including IP address, CNAME, TXT, etc. For our demonstration only a simple domain name to ip address was implemented. The main data structure enabling P2PN-DNS is the distributed hash table. In theory a distributed hash table appears simple but, in practice the implementation of the data structure and overlay network can be very labor and time intensive. There was no reason to reinvent the wheel for P2PN-DNS so, OpenDHT was used for the DHT. OpenDHT aligns well with the scope and goals of P2PN-DNS while bundling in support for other desirable features such as IPv4 and IPv6, public key cryptography, distributed shared keys, and an overall concise, and clear approach. OpenDHT can be found at https://github.com/savoirfairelinux/opendht Since OpenDHT was originally targeted similar distributed applications, the key-value store and commit log functionality were already present and did not have to be implemented independently. The other major facet of P2PN-DNS was the implementation of the DNS protocol. DNS protocol support was based upon a bare bones implementation of DNS called SimpleDNS. The C source code had to be heavily modified to make it compatible with the C++ elements of P2PN-DNS. Most eventenly the use of C++ 11 std::function as callbacks. Additionally, the DNS update (22) message had to be implemented. The DNS update (22) message did not become a public facing feature in the current DNS system and as such it is not included in most DNS resolvers. Systems that do implement a DNS update scheme are called Dynamic Dns or (DDNS) and have accompanying attenication to update a DNS record. (DDNS) is not directly compatible with standard DNS and relies upon an authoritative entity to make the DNS update to the rest of the Domain Name System. In the interests of time and simplicity, a minimal DNS update scheme was implemented. This shortcoming will be rectified in future releases. Please see for the original SimpleDNS source code at https://github.com/mwarning/SimpleDNS The ordering of the updates is determined strictly based upon timestamp in this release. This timestamp method works as long as all the nodes are time synced. This is not a valid assumption in a real world environment as nodes may be operating on seperate networks with differing time sources. In future releases a hash bashed ordering scheme as mentioned in Section 3 will be investigated. The source code for the P2PN-DNS and further documentation can be found at: https://github.com/P2PN-DNS/P2PN-DNS V. Results and AnalysisExperiment and EvaluationThere are a few metrics that matter in a real world application such as the response time to DNS queries, service availability in lieu of loss of nodes, and the average time required to sync records across nodes. For our experiments we propose to examine these metrics and compare, where applicable, to the current DNS system. Dnsmasq will serve as the comparison benchmark as it is a widely deployed DNS forwarder for DNS queries. To measure the response time of the nodes to a DNS request, a DNS request was sent to a P2PN-DNS node via the dig utility, and the record response was recorded. The time taken from request to response was recorded by Wireshark. The same method was employed against DNS(dnsmasq). Five common domain names were chosen; google.com, Amazon.com, Nytimes.com, alibaba.com, and stackoverflow.com. Three samples were taken for each domain name on both P2PN-DNS and dnsmasq to establish a trend and to compensate for outliers. This resulted in 15 queries per dnsmasq and 15 queries for P2PN-DNS. Dnsmasq was running on same network as P2PN-DNS. P2PN-DNS records were added via the DNS update message prior to running the experiment. Figure 5: Query Response Times of DNS versus P2PN-DNS The first request for each domain name to Dnsmasq was of longer duration.This was due to Dnsmasq requesting current information from a higher tier DNS server( Google DNS 8.8.8.8). The second and third requests were served from the Dnsmasq local cache, hence the much shorter duration in response time. P2PN-DNS had slightly longer response time than the Dnsmasq cached response but, less than the first non cached result. This behavior was to be expected as P2PN-DNS has to search the DHT on every lookup and currently does not have local caching enabled. Response time of P2PN-DNS could be improved by enabling local caching in future releases. To test to the availability of P2PN-DNS in lieu of loss of nodes, is to observe the behavior of the P2PN-DNS nodes when nodes disappear from the network. To conduct this experiment, five nodes were started. A DNS record update message was sent to one of the nodes. The nodes were allowed to reach a consistent state where all nodes are informed of the updated record as observed by querying each node for the updated record. One node was be taken offline at a time. Each time a node was removed, the DNS records were checked for consistency between the remaining nodes. Figure 6: Progression of loss of nodes to test availability The DNS record was returned correctly even after only one node of the original five remained. This shows that the P2PN-DNS can fulfill the availability and partition tolerance guarantees even as nodes are removed from the network. To observe the amount of time required to sync records across nodes, the timestamp in the commit log was compared between two different nodes. The difference in the arrival timestamp between the two nodes is directly correlated to the time required to sync records. To ensure accurate timestamping, all nodes were tied to an common clock and synced with Network Time Protocol (NTP). Figure 7: Syslog output from two nodes In the figure above, the syslog output of two nodes can be observed. To produce these results, a DNS update packet was sent to one node. Syslog was monitored for logs containing information about the record update between the two nodes. The update to a record can be seen in one node and in less than one second later, the update is available on the other node. Network capabilities have an impact on the overall record sync time and can affect time jitter. The network variability(jitter) timing issue was avoided by running the nodes on the same network with a common NTP server. VI. ConclusionWith our implementation of a distributed DNS we were able to find a workable tradeoff between consistency, availability, and partition tolerance. By conducting thorough experiments and analysis, we showed that P2PN-DNS can guarantee eventually consistency, availability, and partition tolerance while also providing a comparable amount time required to process DNS queries as compared to existing DNS solutions such as DNSmasq.P2PN-DNS was shown to be available even as nodes were removed giving credence to the claim that P2PN-DNS would be less susceptible than current DNS to regional outages or purposeful attacks.Additionally, the distributed nature of P2PN-DNS makes it applicable for software containers which would benefit from a fault tolerant solution such as P2PN-DNS. VII. References[1] “Domain Name System”. En.wikipedia.org. N.p., 2017. Web. 2 November 2017.https://en.wikipedia.org/wiki/Domain_Name_System[2] “Distributed Denial of Service Attacks on Root Nameservers ”. En.wikipedia.org. N.p., 2017. Web. 2 November 2017. https://en.wikipedia.org/wiki/Distributed_denial-of-service_attacks_on_root_nameservers[3] “I Fought My ISPS Bad Behavior and Won”. Eric Helgeson. Web. 2 November 2017. https://erichelgeson.github.io/blog/2013/12/31/i-fought-my-isps-bad-behavior-and-won/[4] Eckersley, Technical Analysis by Peter. “Widespread Hijacking of Search Traffic in the United States.” Electronic Frontier Foundation, 14 Oct. 2011. Web. 2 November 2017. https://www.eff.org/deeplinks/2011/07/widespread-search-hijacking-in-the-us[5] “Transaction Log”. En.wikipedia.org. N.p., 2017. Web. 2 November 2017. https://en.wikipedia.org/wiki/Transaction_log[6] “Merkle Tree”. En.wikipedia.org. N.p., 2017. Web. 2 November 2017.https://en.wikipedia.org/wiki/Merkle_tree[7] Kangasharju, Jussi. Chapter 4: Distributed Systems: Replication and Consistency. N.p., 2017. Web. 7 November 2017. https://www.cs.helsinki.fi/webfm_send/1256[8] “Blockchain”. En.wikipedia.org. N.p., 2017. Web. 7 November 2017. https://en.wikipedia.org/wiki/Blockchain[9] Jacquin, Ludovic, et al. “The Trust Problem in Modern Network Infrastructures.” SpringerLink, Springer, Cham, 28 Apr. 2015. N.p., 2017. Web. 7 November 2017. https://link.springer.com/chapter/10.1007/978-3-319-25360-2_10[10] “ACID”. En.wikipedia.org. N.p., 2017. Web. 8 November 2017. https://en.wikipedia.org/wiki/ACID[11] DataStax Academy Follow. “A Deep Dive Into Understanding Apache Cassandra.”LinkedIn SlideShare, 25 Sept. 2013. N.p., 2017. Web. 2 December 2017. https://www.slideshare.net/planetcassandra/a-deep-dive-into-understanding-apache-cassandra[12] “Peer-to-Peer (P2P) Systems.” SlidePlayer. N.p., 2017. Web. 2 December 2017 http://slideplayer.com/slide/4168557/[13] “Non-Transitive Connectivity and DHTs “https://www.usenix.org/legacy/events/worlds05/tech/full_papers/freedman/freedman_html/index.html[14] “Amazon Route 53”. En.wikipedia.org. N.p., 2017. Web. 6 December 2017.https://en.wikipedia.org/wiki/Amazon_Route_53[15] “Distributed hash table” .En.wikipedia.org. N.p., 2017. Web. 6 December 2017. https://en.wikipedia.org/wiki/Distributed_hash_table[16] “FIFO (computing and electronics)” .En.wikipedia.org. N.p., 2017. Web. 6 December 2017. https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)[17] “RFC 1034 DOMAIN NAMES - CONCEPTS AND FACILITIES”, N.p., 2017. Web. 2 November 2017. https://www.ietf.org/rfc/rfc1034.txt[18] “RFC 1035 DOMAIN NAMES - IMPLEMENTATION AND SPECIFICATION” , N.p., 2017. Web. 2 November 2017. https://www.ietf.org/rfc/rfc1035.txt[19] “Embedded DNS server in user-defined networks” N.p., 2017. Web. 2 November 2017. https://docs.docker.com/engine/userguide/networking/configure-dns/[20] “OpenDHT” Web. 2 November 2017. https://github.com/savoirfairelinux/opendht[21] “SimpleDNS: Web. 2 November 2017. https://github.com/mwarning/SimpleDNS[22] “ Dynamic Updates in the Domain Name System (DNS UPDATE)” Web. 2 November 2017. https://tools.ietf.org/html/rfc2136[23] “ You can’t Peer to Peer the DNS”https://nohats.ca/wordpress/blog/2012/04/09/you-cant-p2p-the-dns-and-have-it-too/[24] “DNS Server”https://gitlearning.wordpress.com/2015/06/23/dns-server/[25] “Internet censorship in Iran” https://en.wikipedia.org/wiki/Internet_censorship_in_Iran[26] “Great Firewall”https://en.wikipedia.org/wiki/Great_Firewall[27] “ Amazon Route 53”https://en.wikipedia.org/wiki/Amazon_Route_53[28] “DC/OS” https://dcos.io/[29] “Kad Node” https://github.com/kadtools/kad","categories":[],"tags":[{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Research","slug":"Research","permalink":"http://questionableengineering.com/tags/Research/"},{"name":"Distributed Computing","slug":"Distributed-Computing","permalink":"http://questionableengineering.com/tags/Distributed-Computing/"}]},{"title":"Hexo Asset Posts Work Around","slug":"Hexo-Asset-Posts-Work-Around","date":"2017-12-27T01:15:57.000Z","updated":"2023-09-02T13:40:59.855Z","comments":true,"path":"2017/12/26/Hexo-Asset-Posts-Work-Around/","link":"","permalink":"http://questionableengineering.com/2017/12/26/Hexo-Asset-Posts-Work-Around/","excerpt":"Even though Hexo suports markdown, the current asset folders implmention is hacky at best.It does not allow for additional fields. Without the asset_path or image tag your image will often not show up on the index page.The following code will allow you to set properties and display the image on the index page &lt;img src=&quot;&#123;% asset_path image_0.png %&#125;&quot; style=&quot;width: 90%;&quot;/&gt; A better way would be to include the markdown file in the Asset Folder.","text":"Even though Hexo suports markdown, the current asset folders implmention is hacky at best.It does not allow for additional fields. Without the asset_path or image tag your image will often not show up on the index page.The following code will allow you to set properties and display the image on the index page &lt;img src=&quot;&#123;% asset_path image_0.png %&#125;&quot; style=&quot;width: 90%;&quot;/&gt; A better way would be to include the markdown file in the Asset Folder.","categories":[],"tags":[{"name":"Fixes","slug":"Fixes","permalink":"http://questionableengineering.com/tags/Fixes/"},{"name":"Hexo","slug":"Hexo","permalink":"http://questionableengineering.com/tags/Hexo/"}]},{"title":"Investigation of Memory Dependence Strategies","slug":"Investigation-of-Memory-Dependence-Strategies","date":"2017-09-16T01:43:08.000Z","updated":"2023-09-02T16:14:27.100Z","comments":true,"path":"2017/09/15/Investigation-of-Memory-Dependence-Strategies/","link":"","permalink":"http://questionableengineering.com/2017/09/15/Investigation-of-Memory-Dependence-Strategies/","excerpt":"Computer ArchitectureInvestigation of Memory Dependence Prediction Strategies with SimpleScalarLorenzo Allas, John Grun, Sanandeesh Kamat 0.0 AbstractA dynamically scheduled processor may default to in-order execution of Load/Store instructions to avoid Memory Order Violations. This is because, loads executed out of order may be dependent upon prior stores, the addresses of which were initially unknown. To overcome the potentially wasted clock cycles of conservatively stalled loads, known as False Dependencies, Memory Dependence Predictor (MDP) schemes have been developed. This paper demonstrates the implementation of two experimental MDP schemes, Store Sets and Counting Dependence Predictor (CDP) within the SimpleScalar framework. In addition, it demonstrates two baseline MDP schemes, No Speculation and Naive Speculation. The conceptual overview, the software implementation details, as well as quantitative simulation results are provided. The performance of these MDP schemes has been evaluated in terms of three metrics: the number of Memory Order Violations, the number of False Dependencies, and the average IPC. Although the results did not indicate a performance enhancement in terms of execution time, they do demonstrate expected behavior in terms of Memory Order Violations and False Dependencies. Possible implementation shortcomings, and future alterations are later proposed.","text":"Computer ArchitectureInvestigation of Memory Dependence Prediction Strategies with SimpleScalarLorenzo Allas, John Grun, Sanandeesh Kamat 0.0 AbstractA dynamically scheduled processor may default to in-order execution of Load/Store instructions to avoid Memory Order Violations. This is because, loads executed out of order may be dependent upon prior stores, the addresses of which were initially unknown. To overcome the potentially wasted clock cycles of conservatively stalled loads, known as False Dependencies, Memory Dependence Predictor (MDP) schemes have been developed. This paper demonstrates the implementation of two experimental MDP schemes, Store Sets and Counting Dependence Predictor (CDP) within the SimpleScalar framework. In addition, it demonstrates two baseline MDP schemes, No Speculation and Naive Speculation. The conceptual overview, the software implementation details, as well as quantitative simulation results are provided. The performance of these MDP schemes has been evaluated in terms of three metrics: the number of Memory Order Violations, the number of False Dependencies, and the average IPC. Although the results did not indicate a performance enhancement in terms of execution time, they do demonstrate expected behavior in terms of Memory Order Violations and False Dependencies. Possible implementation shortcomings, and future alterations are later proposed. 1.0 Introduction1.1 The Question: To Issue Load or not to Issue Load?In a pipelined In-Order execution processor, if an instruction is dependent upon the result of a previously issued instruction then entire processor pipeline must be stalled. This has the effect of drastically reducing the throughput of the processor by, stalling later instructions that have no dependence upon the stalling instruction. To circumvent the performance limitations inherent in the In-Order pipelined processor designs, dynamic scheduling (Out of Order execution) was introduced. Dynamic scheduling works by allowing instructions to issue out of order. Thus if an instruction is issued and is dependent upon the result of a previous instruction, later instructions do not need to wait. Later non-dependent instructions are allowed to issue as long as the processor has available resources (e.g. Adder, Multiplier, FPU, etc.). Inconveniently, the target memory addresses of memory access instructions (i.e. load/store) are not resolved until after issue. Therefore, earlier implementations of dynamic scheduling (e.g.Tomasulo) issued loads and stores in program order to prevent memory order violations. Memory Order Violations occur when loads and store operate on the same memory address in the incorrect order and thus produce incorrect program execution. While this method ensured the correct program execution, the benefits of dynamic scheduling were not realized for load and store instructions. Additionally, any instructions that are dependent have to wait for the Load or store operation to complete even if disperse loads and stores do not operate on the same memory address. In order to maximize performance gains, researchers began experimenting with schemes to allow for out of order execution of loads and stores. In this paper we shall evaluate two such schemes: Store Sets, and Counting Dependency Predictors. 1.2 The Answer: Memory Dependence Prediction SchemesWhen issuing a load out of program order, it is assumed that the load does not share an address with (i.e. depend upon) any stores which it has overtaken. Therefore, to issue loads out of program order while target addresses are unavailable, the processor requires Memory Dependence Prediction (MDP). This is very similar to Branch Prediction in that the processor guesses on a decision, detects a mishap, recovers state, and learns to avoid the same mistake on future encounters. The two baseline (i.e. corner-cases) MPD schemes are No Speculation and Naive Speculation. Under the terms of No Speculation, no ready loads will queue unless there are no non-ready stores behind it. Under the terms of Naive Speculation, loads will queue as soon as they are ready regardless of the number of non-ready loads behind it. Figure 2 illustrates the concepts of these two schemes. No Speculation and Naive Speculation represent the most conservative and the most aggressive MDP schemes, respectively. Under the terms of the Store Sets algorithms, the processor incrementally logs the PCs of stores upon which loads have historically depended to determine the earliest point in time at which a given load may issue. As conflicting stores are first encountered (detected by Memory Order Violations), their PCs are added to the Store Set to improve future performance. Figure 3 shows illustrates this concept. Today, distributed systems within which centralized fetch and execution streams are infeasible pose a complication for MDP schemes such as Store Sets. To accommodate distributed systems for which memory dependence predictors do not have global knowledge stores at the full program level, the Counting Dependence Predictor (CDP) scheme predicts the number of stores (not specific PCs) which a load must wait for before it is issued. Moreover, the CDP can default the behavior of a given load to No Speculation (conservative) or Naive Speculation (aggressive) depending on how well it performs at run time. A state machine shown in Figure 4 prescribes the behavior of a given load and is designed to maximize overall performance without requisite maintenance of global store information. 1.3 The Purpose of this ProjectThe purpose of this project was to extend the SimpleScalar’s sim-outorder simulator to investigate the effectiveness of Store Sets and CDP as MDP schemes. This required familiarization with the SimpleScalar/sim-outorder source code as well as with the selected MDP schemes. Practical feasibility (e.g. memory/power economy) was not a concern of this simulation-driven project, and so the presented implementations represent idealized behavior with unrestricted architectural resources. 2.0 Methods &amp; Materials2.1 Overview of the SimpleScalar Out-of-Order SimulatorThis project utilized the SimpleScalar Toolset to design/evaluate MDP schemes. The SimpleScalar toolset is divided into modules which are applicable to different types/levels of architectural analysis. Because this project was investigating a type of dynamic scheduling, the sim-outorder module was used. Conveniently, the sim-outorder software is solely confined to the file,simoutorder.c. Sim-outorder centers around the Register-Update-Unit(RUU) and Load-Store-Queue (LSQ) which allow instructions to issue/execute out of order but retire in order. Figure 1 illustrates the pipeline of sim-outorder. The RUU and LSQ are themselves simply arrays of the RUU_Station type, which is container for status information of in-flight instructions. Figure 1: Pipeline for sim-outorder with Memory Dependence Management Highlighted 2.1.1 Memory Dependence Management with Load-Store-Queue RefreshMemory operations are split into two separate instructions: the addition to compute the effective address and the memory operations itself. The Load-Store-Queue Refresh function (lsq_refresh()), indicated in Figure 1, is an array of RUU_Stations of exclusively loads and stores. It’s functionality is to facilitate memory dependence checking and safe issuing of loads (stores are issued in ruu_issue()). Therefore, lsq_refresh() represented the entry point for most of the software developed for this project. In fact, seach MDP scheme implemented in this project is represented entirely by a variant of lsq_refresh() which is selectively called in it’s stead (See Section 2.3 for details). By default,lsq_refresh() stalls any ready load if there exists an earlier store with an unresolved address in the LSQ. If however, the address is ready but the operands are not, lsq_refresh() will track the the store’s effective address and stall any ready load only if their addresses match. As will be shown next, this is a relaxed form of No Speculation combined with a rudimentary form of memory dependence checking which Store Sets extends across multiple clock cycles. 2.2 Overview of the Performance MetricsThe three metrics by which an MDP scheme is evaluated are (1) the Number of Memory Violations, the (2) Number of False Dependencies which have occurred during a program’s execution and the average (3) Instructions Per Cycle. Memory Order Violation program error in which an out-of-order load loads a value before a prior store with a matching effective address completes storing its value to that address. This requires flushing the pipeline and recovering the processor to the state at the point of the offending load. False Dependency program slow-down in which a ready load is stalled due to the detection of a prior unready store which does not have a matching effective address. This results in wasted clock cycles which reduces program execution speed. Instructions Per Cycle (IPC) The average number of instructions which are retired per cycle. In multiple-issue processors like SimpleScalar, this can easily rise above 1. 2.3 Implementation of the MDP Schemes and Metrics AcquisitionImplementing the MDP schemes primarily involved altering the actions taken during lsq_refresh(). Specifically, what to do in the event of a detected unready store and ready load. By default, simoutorder does not risk the possibility of Memory Order Violations. Moreover, the functionality to track the number of False Dependencies did not exist. Therefore, this project also involved developing code detect/track the events of Memory Order Violations and False Dependencies, which can be found in check_mem_violation() and countNumFalseDependencies(), respectively. Figure 2 : Logic for Memory Dependence Algorithms MDP Scheme Memory Order Violations False Dependencies Project Function Name CLI Default None Many lsq_refresh() 0 No Speculation None Many lsq_refresh_NoSpeculation() 1 Naive Speculation Many None lsq_refresh_NaiveSpeculation() 2 Store Sets Few Few lsq_refresh_InfStoreSets() 3 CDP Few Few lsq_refresh_CountingDependencePredictor() 4 Table 1: Expected relative behavior of algorithms 2.3.1 No Speculation (Conservative)For an algorithm which performs no speculation, the load instructions are dispatched to the memory system only when the addresses of all previous stores are known and the operands of those stores are ready. This configuration successfully avoids memory dependence violations entirely by ensuring memory instructions are issued in program order, but provides no prevention against false memory dependencies (see Table 1). As such, this conservative algorithm served as the baseline memory dependence management scheme against which subsequently implemented prediction schemes were compared for maximum false dependencies. 2.3.2 Naive Speculation (Aggressive)The naive prediction algorithm assumes no memory dependencies among store/load instructions. All load and store instructions are issued as soon as possible. This configuration is the opposite of no speculation in that no false dependencies occur, but maximum amount of memory violations are incurred. As such, this aggressive algorithm served as the baseline MDP scheme against which subsequently implemented MDP schemes were compared for maximum memory violations. Functionality to flush the pipeline when a memory violation occurs was not implemented in this simulation due to time constraints. ###2.3.3 Infinite Store SetsThe Store Sets algorithm predicts future memory violations based on their previous occurrences. Each load is initialized to behave according to Naive Speculation, in that it assumes it can issue as soon as it is able to. Upon detection of a memory order violation, the conflicted store and load relationship is saved into a table for future reference. This table is known as a Store Set. During a queue refresh, each ready load’s Store Set is searched for a match (i.e. conflict) with any unready store currently behind it. If a conflict is found, the load is stalled until the matching store is no longer on the LSQ. Because no limits are imposed upon (1) the number of stores a load’s store set can contain, or (2) the number of store sets within which a unique store PC can exist, this implementation is considered to be an Infinite Store Set. These limits do exist in practical implementations which were not considered in this project. Figure 2 illustrates the simplified Infinite Store Sets concept implemented in this project. For the project implementation, the Store Set Index is a C struct and the Store Set itself is simply a C array of addresses. Figure 3: Infinite Store Sets 2.3.4 Counting Dependence PredictionThe counting dependence prediction algorithm uses a state machine for each unique load to determine the correct course of action. Unlike the Store Sets algorithm, the CDP algorithm does not maintain a record of specific Store PCs. Rather, it logs the number of stores which a load must wait for after being ready. This layer of detachment makes CDP an attractive MDP scheme for distributed systems within which globally broadcasted information may not be feasible.Similar to the store set algorithm, each load is initialized to behave according to Naive Speculation (i.e.Aggressive 00 ). As soon as a Memory Order Violation is detected, the state changes to No Speculation (i.e. Conservative. Figure 4: Counting Dependence Predictor State Machine Diagram As long as there is determined to be &gt;1 prior stores upon which this load depends (i.e. a Match ), the load will remain Conservative. As soon as there is determined to be 0 or 1 matching stores, the state will change to One-Store and volley between 01 or 11, respectively. If at any time, however, a Memory Order Violation is detected, the load’s CDP state will return to Conservative. Figure 2 illustrates the CDP concept implemented in this project. For the project implementation, CDP Index is a C struct and the CDP state itself is simply a C enum comprising of the four aforementioned states. 2.3.5 Memory Order Violation and False Dependency DetectionBecause both Store Sets and CDP are initialized/updated by the event of Memory Order Violations, the project’s check_mem_violation() served three simultaneous purposes. Flags Memory Order Violations: issued loads the address of which conflicts with an unexecuted store(s). Initializes/Updates the Store Set of the Offending Load Initializes/Updates the CDP of the Offending Load Therefore, although check_mem_violation() is ostensibly merely metric tracker, it also completes the implementation of Store Set and CDP with state feedback The algorithm for Memory Order Violation detection is shown in Figure 5. What allows this algorithm to be effective is that it is called within RUU_Issue() specifically when a ready load is about to be executed. The False Dependency detection function ( countNumFalseDependencies() ), however, is purely a metric tracker and does not alter the state of ongoing Store Sets or CDP state structures. It is called immediately after the LSQ is refreshed. As shown in Figure 5, it simply counts the number of ready loads which come after an unready store. This is a definition of False Dependency which applies closely to No Speculation, but is loosely applicable to the other MDP schemes. Figure 5: Logic for Memory order Violation Check and False Dependency Check 2.4 Implementation of the SimulationsThe following test programs were run using the aforementioned MDP schemes. Test Programs anagram test-args test-dirent test-fmath test-llong test-lswlr test-printf Table 2: Test programs used in the experiment SimpleScalar Parameter Val Instruction Fetch Queue Size (in inst/s) 4 Instruction Decode Width (insts/cycle) 4 Instruction Issue B/W (insts/cycle) 4 Instruction Commit B/W (insts/cycle) 4 Memory Access Bus Width (in bytes) 8 Register Update Unit Size 8 Load/Store Queue Size 4 Table 3: Relevant Default Parameters for the Simulations In order to specify the MDP scheme to run, additional code was written to selectively invoke a different lsq_reshresh_*() depending on the command line arguments as follows: ./sim-outorder - ALGORITHM_TYPE 0 ./tests/bin/* // 0. Default SimpleScalar Behavior ./sim-outorder -ALGORITHM_TYPE 1 ./tests/bin/* // 1. No Speculation ./sim-outorder -ALGORITHM_TYPE 2 ./tests/bin/* // 2. Naive Speculation ./sim-outorder -ALGORITHM_TYPE 3 ./tests/bin/* // 3. Store Sets ./sim-outorder -ALGORITHM_TYPE 4 ./tests/bin/* // 4. Counting Dependence Predictor By invoking the -redir:sim command line argument simulation outputs were automatically logged to text files. These text files were generated for every combination of test program and MDP scheme, including Default. This resulted in different simulation output text files each of which contain the three principal performance metrics: Number of Memory Violations, Number of False Dependencies, and Average IPC. These results are shown in the next section. 3.0 Results3.1 Instructions Per Cycle (IPC)The IPC is most direct measure of overall program performance. According to Figure 6, the various MDP schemes applied to the test data did not result in significant variation in IPC. Because the simulation parameters were fixed solely as described in Table 3, it is possible that these results would have shown greater variance if B/Ws were increased. Nonetheless, there was a consistent decrease in IPC for No Speculation which is by definition the most sluggish of all MDP schemes. MDP Scheme \\ Program args dirent fmath llong lswlr Math printf Default 0.4638 0.3924 0.7803 0.6043 0.3613 0.9452 1.4645 No Spec 0.4635 0.3923 0.7778 0.6030 0.3611 0.9410 1.4531 Naive Spec 0.4641 0.3924 0.7803 0.6046 0.3613 0.9454 1.4658 Store Sets 0.4641 0.3924 0.7803 0.6045 0.3613 0.9453 1.4645 CDP 0.4610 0.3886 0.7773 0.3701 0.3588 0.8011 0.5214 Table 4: Raw IPC Across Test Programs and MDP Schemes Figure 6: Plotted IPC Across Test Programs and MDP Schemes 3.2 Number of Memory Order ViolationsThe number of Memory Order Violations generated by the simulations was largely consistent with the initial hypothesis. This is in that the Default, and No Speculation MDP schemes consistently resulted in zero Memory Order Violations. This verifies the project’s implementation of the check_mem_violation() function. By design, the Store Sets and CDP algorithm are intended to incur a few number of Memory Order Violations while affording an enhanced IPC. Because the results of the previous section indicated no IPC enhancements, sadly, we merely have only the predicted Memory Order Violation incursion. MDP Scheme \\ Program args dirent fmath llong lswlr Math printf Default 0 0 0 0 0 0 0 No Spec 0 0 0 0 0 0 0 Naive Spec 3 0 5 15 0 45 1745 Store Sets 3 0 5 6 0 17 26 CDP 2 0 4 2 0 8 3 Table 5 : Memory Violation Count Across Test Programs and MDP Schemes Figure 7: Plotted Memory Violation Count Across Test Programs and MDP Schemes 3.2 Number of False DependenciesThe number of False Dependencies generated by the simulations was also largely consistent with the initial hypothesis. This is in that the Naive Speculation consistently resulted in zero False Dependencies. In addition the No Speculation MDP scheme resulted in the largest number of False Dependencies. This verifies the project’s implementation of the countNumFalseDependencies() function as well as baseline MDP schemes. It is optimistic that the Store Sets and CDP schemes resulted in fewer False Dependencies than the Default and No Speculation. However, as there was no significant improvement in IPC, the overall value of these experimental MDP schemes is still undemonstrated. MDP Scheme \\ Program args dirent fmath llong lswlr Math printf Default 3 0 88 55 0 158 3342 No Spec 436 261 1300 565 351 2124 35342 Naive Spec 0 0 0 0 0 0 0 Store Sets 0 0 3 17 0 52 3591 CDP 17 24 57 22 27 63 27 Table 6: False Dependency Count Across Test Programs and MDP Schemes Figure 8: Plotted False Dependency Count Across Test Programs and MDP Schemes 4.0 DiscussionThe most significant metric which justifies an algorithm’s utility is the IPC. Because these results did not demonstrate a significant enhancement of IPC for either of the two experimental MDP schemes (Store Sets and CDP), their implementations cannot be proclaimed entirely successful. However, there were several aspects of the results which did support the correctness of their implementations and their consistency with theory. For example: As expected, the Default and No Speculation MDPs generated no Memory Order Violations and the Naive Speculation MDP many Memory Order Violations. As expected, the Default and No Speculation MDPs generated significant number of False Dependencies and the Naive Speculation MDP generated no False Dependencies. These facts verify the implementations of the baseline MDPs. For the experimental MDPs, as expected, the Store Sets and CDP did generate Memory Order Violations, which is the trigger event by which the Store Sets and CDPs are to be initialized in the first place. Furthermore, as expected, number of False Dependencies generated by Store Sets and CDP are fewer than those of Default, No Speculation, and Naive Speculation. The various parameters which dictate the width of instructions queueing/decoding/issuing/committing etc. were fixed for all simulations at either 4 or 8 (See Table 3). Because MDP schemes are intended to yield greater dividends for higher bandwidth processors, it is possible that increasing these parameters would reveal inter-MDP scheme variation in IPC. A follow on study in which the parameters of Table 3 are modulated could demonstrate this. Nonetheless there are a few implementation features of Store Sets, CDP, and metric tracking which were either approximated here or entirely foregone. For example, although the mechanism to detect Memory Order Violations was implemented, the mechanism to recover processor state to the point of the offending load was not. This mechanism would be entirely analogous to that of processor state recovery during branch mis-prediction. The reason no such MDP recovery mechanism existed at first is that the default implementation of SimpleScalar does not allow the possibility of Memory Order Violations at all (See Figure 1). What this should mean is that every Memory Order Violation encountered here caused a programmatic error. However, sim-outorder prints the expected output of each simulated program adjacent to the generated output. Throughout all 40 simulation runs, no differences were seen between the expected and generated outputs. Although the reason for this lack of discrepancy is unknown, it does raise the possibility that SimpleScalar was somehow detecting the Memory Order Violations and recovering processor state. If this is so, the additional clock cycles cost from recovery were already accounted for in the provided results. If not, the implementations provided here are certainly incomplete and represent optimistic IPCs in that the penalty clock cycles of MDP recovery were not accounted for. One last consideration is that the provided implementations did not strive for minimal memory usage in anyway. For instance, the Store Sets and CDP here maintained a separate index for each load. In practice, much like with branch prediction, the CDP and Store Sets would use reduced table sizes for loads to hash into, and not necessarily track all loads across the entire program execution. 5.0 ConclusionBecause the effective addresses of Loads and Stores cannot always be known at the issue stage, dynamic scheduling processors have traditionally defaulted them to in-order scheduling to avoid Memory Order Violations. To exploit more ILP and reduce False Dependencies, Memory Dependence Prediction (MDP) schemes have been developed. This project sought to demonstrate the performance enhancement capabilities of two such MDP schemes, Store Sets and Counting Dependence Predictor (CDP), within the SimpleScalar simulation framework. SimpleScalar is an industry standard simulator and has been independently verified. Carrying out this project required the team’s thorough familiarization with the MDP scheme concepts as well as the SimpleScalar source code. The project’s developed source code was peer reviewed by the members of the group and was submitted for further investigation. The project’s developed source code was submitted with built in functionality to toggle between four different MDP schemes (plus Default); two baseline, and two experimental. The two baseline MDPs, No Speculation and Naive Speculation, successfully demonstrated the predicted behavior of maximizing False Dependencies and Memory Order Violations, respectively. Moreover, the Store Sets and CDP implementations demonstrated an expected moderate incurrence of Memory Order Violations and False Dependencies. However, the Store Sets and CDP did not demonstrate a significant enhancement of IPC; one of the primary benchmarks of an MDP scheme’s utility. Possible explanations for this result include improper input parameter settings detailed in Table 3. Because MDPs are intended for wide-issue processors, it is possible that these particular set of parameters were insufficient to reveal the intended benefits. Moreover, CDP is necessarily a more handicapped version of Store Sets that would only be functionally relevant if SimpleScalar were implemented as a distributed system. This project enabled the group members to not only learn about but take on Computer Architecture research through the power of modelling &amp; simulation. Carrying out this project allowed us to combine architectural theory with hands-on quantitative performance analysis. Doing so allowed us to act in the capacity of, not only students, but designers. 6.0 References[1] G. Z. Chrysos and J. S. Emer. Memory dependence prediction using store sets. In Proceedings of the 25th Annual International Symposium on Computer Architecture, ISCA ‘98, pages 142{153, Washington, DC, USA, 1998. IEEE Computer Society[2] D. Burger, T. M. Austin. The SimpleScalar Tool Set, Version 2.0. [3]F. Roesner, D. Burger, and S. W. Keckler. Counting dependence predictors. In Proceedings of the 35th Annual International Symposium on Computer Architecture ISCA ‘08, pages 215{226, Washington, DC, USA, 2008. IEEE Computer Society.","categories":[],"tags":[{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Research","slug":"Research","permalink":"http://questionableengineering.com/tags/Research/"}]},{"title":"Nodejs 6 on Ubuntu 16.04 LTS","slug":"Nodejs-6-on-Ubuntu-16-04-LTS","date":"2017-09-10T00:52:48.000Z","updated":"2017-12-27T04:17:49.000Z","comments":true,"path":"2017/09/09/Nodejs-6-on-Ubuntu-16-04-LTS/","link":"","permalink":"http://questionableengineering.com/2017/09/09/Nodejs-6-on-Ubuntu-16-04-LTS/","excerpt":"Nodejs 6 on Ubuntu 16.04 LTSERROR : Buffer.alloc is not a function","text":"Nodejs 6 on Ubuntu 16.04 LTSERROR : Buffer.alloc is not a function Fix: upgrade nodejs to Version &gt; 5.1 curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | sudo apt-key add - sudo sh -c &quot;echo deb https://deb.nodesource.com/node_6.x yakkety main \\ &gt; /etc/apt/sources.list.d/nodesource.list&quot; sudo apt-get update sudo apt-get install nodejs","categories":[],"tags":[{"name":"Fixes","slug":"Fixes","permalink":"http://questionableengineering.com/tags/Fixes/"},{"name":"NodeJs","slug":"NodeJs","permalink":"http://questionableengineering.com/tags/NodeJs/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://questionableengineering.com/tags/Ubuntu/"}]},{"title":"Analysis of the Kalman Filter Algorithm","slug":"Report","date":"2017-07-07T21:46:15.000Z","updated":"2023-09-04T23:05:28.409Z","comments":true,"path":"2017/07/07/Report/","link":"","permalink":"http://questionableengineering.com/2017/07/07/Report/","excerpt":"MotivationThe Kalman filter finds applications in extracting useful data from inherently noisy sources. One common application is smoothing sensor data. In order for most sensor data to be effectively employed in applications like control loops or navigation systems it must first be filtered in a manner that removes noise but, does not introduce an unacceptable amount of additional error or increases processing and memory load on the system. In this regard the Kalman filter excels. The Kalman filter can also be extended to combine data from multiple input sources to further reduce the error in a signal or sample. This property has many applications in the area of sensor fusion. Kalman filter sensor fusion is commonly used in robotic control systems. A common use case is autonomous vehicle navigation and control eg. Quadcopters or spacecraft. Additionally, the Kalman filter is often used in analog and digital signal processing. The Kalman filter algorithm enjoys implementations in both software and hardware.","text":"MotivationThe Kalman filter finds applications in extracting useful data from inherently noisy sources. One common application is smoothing sensor data. In order for most sensor data to be effectively employed in applications like control loops or navigation systems it must first be filtered in a manner that removes noise but, does not introduce an unacceptable amount of additional error or increases processing and memory load on the system. In this regard the Kalman filter excels. The Kalman filter can also be extended to combine data from multiple input sources to further reduce the error in a signal or sample. This property has many applications in the area of sensor fusion. Kalman filter sensor fusion is commonly used in robotic control systems. A common use case is autonomous vehicle navigation and control eg. Quadcopters or spacecraft. Additionally, the Kalman filter is often used in analog and digital signal processing. The Kalman filter algorithm enjoys implementations in both software and hardware. Describe Algorithm detailsThe Kalman filter algorithm uses multiple input signals samples, often periodically, to increase the overall estimation accuracy of the signal with noise removed. The Kalman filter has 5 sets of variables one must understand. The the first is the self-explanatory unfiltered input signal. It is important to state, the algorithm assumes that all input signals into the filter contain a certain amount of noise variance. The Kalman gain, which is recursively calculated from the variance of the input signal over many samples. The last state (Xk-1) of the system, which is a gain weighted sum of all previous input signal samples. The current state (Xk) , which is an estimate of the expected state of the system as calculated by a function that describes the system/signal. The output signal is a gain weighted sum of the value of the current input signal and the current state estimation of the system. The Kalman filter algorithm uses two main components, a predicative step and a update step. See Figure 1. Figure 1. Simplified process flow representation of input signal and estimation in kalman filter. In the predictive step the Kalman filter relies upon a function model of the system/signal in order to calculate the predicted current state (Xk) from the last state (Xk-1) and any relevant input signals. The system/signal predicted variance is also calculated in this step. In the update step, the next estimated output signal is produced from a gain weighted sum of the estimated current state and the new input signal sample. In the case of a temperature sensor, the Kalman filter would produce a new estimated temperature for the output from the gain weighted sum of previous temperature samples and the new temperature sample. The gain is calculated based upon the variance observed between the predicted variance and the variance of the new sample. The filter is tuned to give a higher weight to samples with lower error. Explain why the chosen algorithms are employed for the problem.The algorithm has found employment as a solution to many common engineering problems such as processing inputs into control systems and for fusing sensor data from multiple sources. The algorithm is popular as a solution to these problems due to its low memory and processing footprint, constant running time, and comparatively simple design as compared to other solutions. The Kalman filter reduces the amount of working memory required by encoding past history of the inputs into a single current state variable (Xk). This encoding reduces the amount of memory consumed by the algorithm as past information does not need to be retained. Additionally, the amount of data processing per signal sample is reduced as only the current state variable and new sample are processed. These reductions in processing and memory are advantages in resource constrained environments, such as embedded systems or hardware implementations. The Kalman filter algorithm also lends itself well to “real time” systems such as robotic control where a predictable delay and constant runtime is required.The Kalman filter algorithm has a O(1) processing complexity and a O(1) memory space complexity. The Kalman filter can also be extended to combine inputs from multiple sources to further reduce signal sample variance. This property lends itself well to applications employing sensor fusion such as inertial navigation units (INUs) Experimental configuration and details.The experiment consists of a force sensitive resistor sensor sampled once every 100 mSec by an Arduino compatible microcontroller(ESP8266). The microcontroller sends the raw sensor samples to the computer over a serial connection. The computer then writes the raw samples to a file on the hard disk (Data.txt). Once a large number of samples have been collected, the raw samples are passed into the Kalman filter(KalmanFilterWrapper.exe). KalmanFilterWrapper.exe produces two output files KALMAN_INPUT_VS_OUTPUT.csv and KALMAN_FILTER_RUNNING_TIME_VS_INPUT_SIZE.csv that contain the raw samples vs the Kalman filtered samples and the running time vs input size respectively. Kalman filters will often have many sensor inputs , process time variant signals, and must account for control inputs. With the addition of more inputs a gain matrix must be maintained that relates each input to each other input in addition to the gain between current Xk and last state Xk-1. These factors quickly drive a simple algorithm into such a complex system that it is used routinely in PHD. thesi. For these reasons, a simple implementation of the Kalman was chosen. Finally, a simpler implementation will clearly show the algorithm operation. The implemented Kalman filter is processing a single variable, static , time invariant signal(Voltage proportional to the weight of Expo marker) In this implementation, the function model of the system/signal is Current state = last state I.e Xk = Xk-n. The test should clearly show noise in raw samples and the resulting filtered output of the Kalman filter. Figure 2: Experimental setup to collect sensor data. EXPO marker used to provide static weight greater than baseline. Figure 3. Schematic of the circuit used to collect data and send information the the computer. Source code and related information to reproduce experiment Arduino code to collect samples: DataStructuresKalmanFilter.ino Schematic of sensor collection: Kalman_sensor_collection_schematic.pdf Kalman filter Source Code see: KalmanFilterWrapper.cpp – Wrapper to process data and produce output files ./KalmanFilter.cpp – Kalman filter implementation Run make in project root to compile C++ source code. make Exec KalmanFilterWrapper.exe to produce outfiles KALMAN_INPUT_VS_OUTPUT.csv KALMAN_FILTER_RUNNING_TIME_VS_INPUT_SIZE.csv All code can be found at https://github.com/JohnGrun/KalmanFilterPaper Sources of Variance in the experimentRaw measurement sources of known varianceThe sensor employed(FSR406) has tolerance of 2% at constant temperature as called out in the data sheet Datasheets_FSR.pdf. At a supply voltage of 3.3v, 2% is ~0.066 Volts of error due to the FSR sensor.See references for link to data sheet. The Esp8266 ADC has a resolution of 12 bits or 4096 divisions. With a Vcc = 3.3V; 4096 = 3.3 Volts, 0 = 0 Volts. A change of 1 in a measurement corresponds to 3.3/4096 = 0.00080566406 Volts. Compared the 0.066 Volts of error introduced by the FSR sensor the ADC resolution is not seen to be significant and, can thus be neglected in calculations. Running time sources of known varianceThe running time of the algorithm may be influenced by external factors such as file system assess, resource allocations, paging, or process scheduling. To compensate for the aforementioned sources of external variances many samples have to be taken. Results and Analysis Figure 4. Sensor samples of proportional voltage to weight of EXPO marker. Raw samples from FSR sensor (Blue). Kalman filtered data of samples (Orange). As can be seen in Figure 4, the raw samples contain a large amount of noise. The standard deviation of the raw samples was 1478.01. A static, time invariant signal( Voltage proportional to the weight of Expo marker) with such a high standard deviation is unusable in real world applications. Once the signal is processed by the Kalman filter, and significant time has passed, the signal become far less variable. The standard deviation of Kalman filtered signal was 98.07, a full order of magnitude less than the raw input signal. Additionally, the filtered signal reached a stable output with low variance as would be expected with a static input signal (Voltage proportional to the weight of Expo marker). Figure 5. Running Time vs Input size. Running time (Blue) of each call to the Kalman filter algorithm using same data in Figure 4. The second graph Figure 5. depicts the running time of the Kalman filter vs the number of samples processed. As expected, the running time is O(1). The Kalman filter only processes one measurement at a time. The output estimation is a weighted sum the past state( all previous samples) and the current measurement state and is updated once per function call. Only for 2 data points out of 5121 samples did the running time change significantly. These 2 data points are likely due to other factors not related to the Kalman filter, such as process scheduling or file system assesses on the host computer. Discussion As can be seen from the results of the experiments the Kalman filter algorithm manages to clean up a noisy signal while running in constant time. There are two limitations of the Kalman filter algorithm. The algorithm requires a finite amount of startup time to reach a output that is representative of the filtered signal see Figure 4. In practice, as long as this startup time can be tolerated, a valid output signal can be extracted. The other limitation is that the Kalman filter algorithm requires knowledge of the process. This is manifested in the prediction stage, where the value of Xk is based upon an equation describing how the system/signal will change over time. E.g. Xk = F(Xkn-1,input1, input2). This makes it difficult to use a Kalman filter algorithm as general case filter, or where the input signal does not have a known function describing its behavior. If one can tolerate these minor shortcomings the Kalman filter algorithm is an excellent choice to clean up noisy input signals with minimal memory and processing overhead. ReferencesThe Extended Kalman Filter: An Interactive Tutorial for Non-Experts. (2017, April 28). Retrieved April 28, 2017, from https://home.wlu.edu/~levys/kalman_tutorial/kalman_01.html Kelly, A. (2006, May 24). A 3D State Space Formulation of a Navigation Kalman Filter for Autonomous Vehicles. Retrieved April 3, 2017, from http://www.frc.ri.cmu.edu/~alonzo/pubs/reports/kalman_V2.pdf Welch, G., &amp; Bishop, G. (2001). An Introduction to the Kalman Filter. Retrieved April 3, 2017, from http://www.cs.unc.edu/~tracker/media/pdf/SIGGRAPH2001_CoursePack_08.pdf Wikipedia. (2017, April 28). Variance. Retrieved April 3, 2017, from https://en.wikipedia.org/wiki/Variance Interlink Electronics. (2017, April 4). FS R ® 400 Series Data Sheet. Retrieved April 4, 2017, from https://www.interlinkelectronics.com/datasheets/Datasheet_FSR.pdf Kohanbash, Y. (2014, January 30). Kalman Filtering – A Practical Implementation Guide (with code!). Retrieved April 29, 2017, from http://robotsforroboticists.com/kalman-filtering/ Wikipedia. (2017, April 06). Kalman filter. Retrieved April 29, 2017, from https://en.wikipedia.org/wiki/Kalman_filter","categories":[],"tags":[{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Research","slug":"Research","permalink":"http://questionableengineering.com/tags/Research/"},{"name":"Kalman","slug":"Kalman","permalink":"http://questionableengineering.com/tags/Kalman/"},{"name":"C++","slug":"C","permalink":"http://questionableengineering.com/tags/C/"}]},{"title":"CNC Conversion Mechanical","slug":"CNC-Conversion-MARS","date":"2014-10-29T18:49:34.000Z","updated":"2023-09-03T15:05:51.546Z","comments":true,"path":"2014/10/29/CNC-Conversion-MARS/","link":"","permalink":"http://questionableengineering.com/2014/10/29/CNC-Conversion-MARS/","excerpt":"","text":"Mini MillHi Torque Mini Mill. Unboxing the kit.CNC FUSION KIT Unfortunately, this company is no longer around. Thankfully, there are other kits available to perform this conversion. Solid Column upgrade The original hi torque mini mill from Little Machine Shop had a gimmicky tilting column. While it was nice to be able to tilt the mill head it was impossible to tram the mill. This solid column is complete replacement for the tilting column. Disassembly X/y stage separated from the rest of the mill. The tilting column version only required removing one large bolt on the back of the mill. Tilting column with z axis Front of the tilting column with the mill head removed. X and Y axis The ballscrew is slightly too long. So we will have to do a little metal removal. The ballscrew hits the other side of the casting Grinding out a small pocket to fit the ballscrew end. This was done with a rotary tool and a mini end mill. Mounting the x ballscrew to the bottom of the x. I believe this mounted using the existing tapped holes. Assembled X and Y table Z axis Tapping the mounting holes for the z axis ballscrew mount The z axis ballscrew is positioned off to the side of the main column. We have to attach a metal plate to the top of the column in order to mount the ballscrew. This requires locating the mounting plate. Using center taps to locate the mounting holes. Drilling the mounting holes. Tapping the mounting holes. Z axis ballscrew mounted on the main column. Metal mounting plate attached to the top of the column. Ballscrew nut attached to the mill head via two mounting holes. Another view of the z axis. ElectronicsController Beagle Bone Black Interface cardC10- PARALLEL PORT INTERFACE CARD Motor ControllersM880A M880Am.pdf Control Board View of the inside of the electronics enclosure. This is a NEMA 4 enclosure the minimum you need when dealing with possible over spray. Electronics mounted in the enclosure. Top left parallel port interface card mounted on standoffs above the beagle bone black. Top right the CNC electronics from the original hi torque mini mill. Left middle - A DC to DC power supply to produce 5v for all 5 volt components including the beagle bone black and parallel port card. Center middle - Stepper motor controllers. Right middle 36 v power supply. Bottom Din rail mounted terminal strip. Spindle ControlThe original mill only had a simple pot to turn for spindle control. This will not work for a CNC that needs to vary the spindle control throughout a job. To correct this short coming we’re going to replace the original spindle control with a 0-10 volt control. This will allow the beagle bone black him costume the books speed and direction via then parallel port adapter. CNC Spindle Control Upgrade Kit, Mini Lathe and Mini Mill 4213CNCSpindleControlUpgradeKit.pdf Mounting the spindle control Wiring the spindle control to the CNC controller board. This is a direct replacement available for this mill from Little Machine Shop. Cnc control board as seen mounted on the enclosure. Electronics Enclosure Mounting the read of the main column View of the connections to the motors and end stops. I also added HDMI, USB, and Ethernet pass throughs to allow connection to the beagle bone. Complete View of machine kit (Linux cnc) running on the beagle bone black. Completed CNC machine Cnc up and running with monitor mounted on side.","categories":[],"tags":[{"name":"CNC","slug":"CNC","permalink":"http://questionableengineering.com/tags/CNC/"}]},{"title":"Large CNC","slug":"Large-CNC","date":"2011-12-04T19:40:20.000Z","updated":"2023-09-02T20:36:43.972Z","comments":true,"path":"2011/12/04/Large-CNC/","link":"","permalink":"http://questionableengineering.com/2011/12/04/Large-CNC/","excerpt":"","text":"","categories":[],"tags":[{"name":"CNC","slug":"CNC","permalink":"http://questionableengineering.com/tags/CNC/"}]}],"categories":[],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://questionableengineering.com/tags/Kubernetes/"},{"name":"Computing","slug":"Computing","permalink":"http://questionableengineering.com/tags/Computing/"},{"name":"Microk8s","slug":"Microk8s","permalink":"http://questionableengineering.com/tags/Microk8s/"},{"name":"Electronics","slug":"Electronics","permalink":"http://questionableengineering.com/tags/Electronics/"},{"name":"LED","slug":"LED","permalink":"http://questionableengineering.com/tags/LED/"},{"name":"WLED","slug":"WLED","permalink":"http://questionableengineering.com/tags/WLED/"},{"name":"Hyperion","slug":"Hyperion","permalink":"http://questionableengineering.com/tags/Hyperion/"},{"name":"RestAPI","slug":"RestAPI","permalink":"http://questionableengineering.com/tags/RestAPI/"},{"name":"Fixes","slug":"Fixes","permalink":"http://questionableengineering.com/tags/Fixes/"},{"name":"Electrical","slug":"Electrical","permalink":"http://questionableengineering.com/tags/Electrical/"},{"name":"CNC","slug":"CNC","permalink":"http://questionableengineering.com/tags/CNC/"},{"name":"Research","slug":"Research","permalink":"http://questionableengineering.com/tags/Research/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://questionableengineering.com/tags/Machine-Learning/"},{"name":"React Native","slug":"React-Native","permalink":"http://questionableengineering.com/tags/React-Native/"},{"name":"OpenScad","slug":"OpenScad","permalink":"http://questionableengineering.com/tags/OpenScad/"},{"name":"Mechanical","slug":"Mechanical","permalink":"http://questionableengineering.com/tags/Mechanical/"},{"name":"Distributed Computing","slug":"Distributed-Computing","permalink":"http://questionableengineering.com/tags/Distributed-Computing/"},{"name":"Hexo","slug":"Hexo","permalink":"http://questionableengineering.com/tags/Hexo/"},{"name":"NodeJs","slug":"NodeJs","permalink":"http://questionableengineering.com/tags/NodeJs/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://questionableengineering.com/tags/Ubuntu/"},{"name":"Kalman","slug":"Kalman","permalink":"http://questionableengineering.com/tags/Kalman/"},{"name":"C++","slug":"C","permalink":"http://questionableengineering.com/tags/C/"}]}